{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Intro","text":"<p> <p> One interface. Every LLM. </p> </p> <p><code>any-llm</code> is a Python library providing a single interface to different llm providers.</p> <pre><code>from any_llm import completion\n\n# Using the messages format\nresponse = completion(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"What is Python?\"}],\n    provider=\"openai\"\n)\nprint(response)\n\n# Switch providers without changing your code\nresponse = completion(\n    model=\"claude-sonnet-4-5-20250929\",\n    messages=[{\"role\": \"user\", \"content\": \"What is Python?\"}],\n    provider=\"anthropic\"\n)\nprint(response)\n</code></pre>"},{"location":"#why-any-llm","title":"Why any-llm","text":"<ul> <li>Switch providers in one line</li> <li>Unified exception handling across providers</li> <li>Simple API, powerful features</li> </ul> <p>View supported providers \u2192</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Get started in 5 minutes \u2192 - Install the library and run your first API call.</p>"},{"location":"#api-documentation","title":"API Documentation","text":"<p><code>any-llm</code> provides two main interfaces:</p> <p>Direct API Functions (recommended for simple use cases): - completion - Chat completions with any provider - embedding - Text embeddings - responses - OpenResponses API for agentic AI systems</p> <p>AnyLLM Class (recommended for advanced use cases): - Provider API - Lower-level provider interface with metadata access and reusability</p>"},{"location":"#for-ai-systems","title":"For AI Systems","text":"<p>This documentation is available in two AI-friendly formats:</p> <ul> <li>llms.txt - A structured overview with curated links to key documentation sections</li> <li>llms-full.txt - Complete documentation content concatenated into a single file</li> </ul>"},{"location":"providers/","title":"Supported Providers","text":"<p><code>any-llm</code> supports the below providers. In order to discover information about what models are supported by a provider as well as what features the provider supports for each model, refer to the provider documentation.</p> <p>Provider source code can be found in the <code>src/any_llm/providers/</code> directory of the repository.</p> <p>Legend</p> <ul> <li>Key: Environment variable for the API key (e.g., <code>OPENAI_API_KEY</code>).</li> <li>Base: Environment variable for a custom API base URL (e.g., <code>OPENAI_BASE_URL</code>). Useful for proxies or self-hosted endpoints.</li> <li>Reasoning (Completions): Provider can return reasoning traces alongside the assistant message via the completions and/or streaming endpoints. This does not indicate whether the provider offers separate \"reasoning models\". See this discussion for more information.</li> <li>Streaming (Completions): Provider can stream completion results back as an iterator.</li> <li>Image (Completions): Provider supports passing an <code>image_data</code> parameter for vision capabilities, as defined by the OpenAI spec here.</li> <li>OpenResponses API: Provider supports the OpenResponses specification for agentic AI systems. See the Responses API docs for usage details.</li> <li>List Models API: Provider supports listing available models programmatically via the <code>list_models()</code> function. This allows you to discover what models are available from the provider at runtime, which can be useful for dynamic model selection or validation.</li> </ul> ID Key Base Responses Completion Streaming(Completions) Reasoning(Completions) Image (Completions) Embedding List Models Batch <code>anthropic</code> ANTHROPIC_API_KEY ANTHROPIC_BASE_URL \u274c \u2705 \u2705 \u2705 \u2705 \u274c \u2705 \u274c <code>azure</code> AZURE_API_KEY AZURE_AI_CHAT_ENDPOINT \u274c \u2705 \u2705 \u274c \u274c \u2705 \u274c \u274c <code>azureopenai</code> AZURE_OPENAI_API_KEY AZURE_OPENAI_ENDPOINT \u2705 \u2705 \u2705 \u274c \u2705 \u2705 \u2705 \u274c <code>bedrock</code> AWS_BEARER_TOKEN_BEDROCK AWS_ENDPOINT_URL_BEDROCK_RUNTIME \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c <code>cerebras</code> CEREBRAS_API_KEY CEREBRAS_API_BASE \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c <code>cohere</code> COHERE_API_KEY COHERE_BASE_URL \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c <code>databricks</code> DATABRICKS_TOKEN DATABRICKS_HOST \u274c \u2705 \u2705 \u2705 \u274c \u2705 \u274c \u274c <code>deepseek</code> DEEPSEEK_API_KEY DEEPSEEK_API_BASE \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c <code>fireworks</code> FIREWORKS_API_KEY FIREWORKS_API_BASE \u2705 \u2705 \u2705 \u2705 \u2705 \u274c \u2705 \u274c <code>gateway</code> GATEWAY_API_KEY GATEWAY_API_BASE \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 <code>gemini</code> GEMINI_API_KEY/GOOGLE_API_KEY GOOGLE_GEMINI_BASE_URL \u274c \u2705 \u2705 \u2705 \u274c \u2705 \u2705 \u274c <code>groq</code> GROQ_API_KEY GROQ_BASE_URL \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c <code>huggingface</code> HF_TOKEN HUGGINGFACE_API_BASE \u2705 \u2705 \u2705 \u274c \u274c \u274c \u2705 \u274c <code>inception</code> INCEPTION_API_KEY INCEPTION_API_BASE \u274c \u2705 \u2705 \u274c \u274c \u274c \u2705 \u274c <code>llama</code> LLAMA_API_KEY LLAMA_API_BASE \u274c \u2705 \u2705 \u274c \u274c \u274c \u2705 \u274c <code>llamacpp</code> None LLAMACPP_API_BASE \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c <code>llamafile</code> None LLAMAFILE_API_BASE \u274c \u2705 \u274c \u2705 \u274c \u274c \u2705 \u274c <code>lmstudio</code> LM_STUDIO_API_KEY LM_STUDIO_API_BASE \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c <code>minimax</code> MINIMAX_API_KEY MINIMAX_API_BASE \u274c \u2705 \u2705 \u2705 \u274c \u274c \u274c \u274c <code>mistral</code> MISTRAL_API_KEY MISTRAL_API_BASE \u274c \u2705 \u2705 \u2705 \u274c \u2705 \u2705 \u2705 <code>moonshot</code> MOONSHOT_API_KEY MOONSHOT_API_BASE \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c <code>nebius</code> NEBIUS_API_KEY NEBIUS_API_BASE \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c <code>ollama</code> None OLLAMA_HOST \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c <code>openai</code> OPENAI_API_KEY OPENAI_BASE_URL \u2705 \u2705 \u2705 \u274c \u2705 \u2705 \u2705 \u2705 <code>openrouter</code> OPENROUTER_API_KEY OPENROUTER_API_BASE \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c <code>perplexity</code> PERPLEXITY_API_KEY PERPLEXITY_BASE_URL \u274c \u2705 \u2705 \u274c \u2705 \u274c \u274c \u274c <code>portkey</code> PORTKEY_API_KEY PORTKEY_API_BASE \u274c \u2705 \u2705 \u2705 \u2705 \u274c \u2705 \u274c <code>sagemaker</code> AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY SAGEMAKER_ENDPOINT_URL \u274c \u2705 \u2705 \u274c \u2705 \u2705 \u274c \u274c <code>sambanova</code> SAMBANOVA_API_KEY SAMBANOVA_API_BASE \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c <code>together</code> TOGETHER_API_KEY TOGETHER_API_BASE \u274c \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c <code>vertexai</code> VERTEXAI_API_BASE \u274c \u2705 \u2705 \u2705 \u274c \u2705 \u2705 \u274c <code>vertexaianthropic</code> VERTEXAI_ANTHROPIC_API_BASE \u274c \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c <code>vllm</code> VLLM_API_KEY VLLM_API_BASE \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c <code>voyage</code> VOYAGE_API_KEY VOYAGE_API_BASE \u274c \u274c \u274c \u274c \u274c \u2705 \u274c \u274c <code>watsonx</code> WATSONX_API_KEY WATSONX_URL \u274c \u2705 \u2705 \u274c \u2705 \u274c \u2705 \u274c <code>xai</code> XAI_API_KEY XAI_API_BASE \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c <code>zai</code> ZAI_API_KEY ZAI_BASE_URL \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 or newer</li> <li>API keys for your chosen LLM provider</li> </ul>"},{"location":"quickstart/#installation","title":"Installation","text":"<pre><code>pip install any-llm-sdk[all]  # Install with all provider support\n</code></pre>"},{"location":"quickstart/#installing-specific-providers","title":"Installing Specific Providers","text":"<p>If you want to install a specific provider from our supported providers:</p> <pre><code>pip install any-llm-sdk[mistral]  # For Mistral provider\npip install any-llm-sdk[ollama]   # For Ollama provider\n# install multiple providers\npip install any-llm-sdk[mistral,ollama]\n</code></pre>"},{"location":"quickstart/#library-integration","title":"Library Integration","text":"<p>If you're building a library, install just the base package (<code>pip install any-llm-sdk</code>) and let your users install provider dependencies.</p> <p>API Keys: Set your provider's API key as an environment variable (e.g., <code>export MISTRAL_API_KEY=\"your-key\"</code>) or pass it directly using the <code>api_key</code> parameter.</p>"},{"location":"quickstart/#apis","title":"APIs","text":""},{"location":"quickstart/#using-the-anyllm-class","title":"Using the AnyLLM Class","text":"<p>For applications making multiple requests with the same provider, use the <code>AnyLLM</code> class to avoid repeated provider instantiation:</p> <pre><code>import os\n\nfrom any_llm import AnyLLM\n\n# Make sure you have the appropriate API key set\napi_key = os.environ.get('MISTRAL_API_KEY')\nif not api_key:\n    raise ValueError(\"Please set MISTRAL_API_KEY environment variable\")\n\nllm = AnyLLM.create(\"mistral\")\n\nresponse = llm.completion(\n    model=\"mistral-small-latest\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\nprint(response.choices[0].message.content)\n\nmetadata = llm.get_provider_metadata()\nprint(f\"Supports streaming: {metadata.streaming}\")\nprint(f\"Supports tools: {metadata.completion}\")\n</code></pre>"},{"location":"quickstart/#api-call","title":"API Call","text":"<pre><code>import os\n\nfrom any_llm import completion\n\n# Make sure you have the appropriate API key set\napi_key = os.environ.get('MISTRAL_API_KEY')\nif not api_key:\n    raise ValueError(\"Please set MISTRAL_API_KEY environment variable\")\n\n# Recommended: separate provider and model parameters\nresponse = completion(\n    model=\"mistral-small-latest\",\n    provider=\"mistral\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"quickstart/#when-to-choose-which-approach","title":"When to Choose Which Approach","text":"<p>Use Direct API Functions (<code>completion</code>, <code>acompletion</code>) when:</p> <ul> <li>Making simple, one-off requests</li> <li>Prototyping or writing quick scripts</li> <li>You want the simplest possible interface</li> </ul> <p>Use Provider Class (<code>AnyLLM.create</code>) when:</p> <ul> <li>Building applications that make multiple requests with the same provider</li> <li>You want to avoid repeated provider instantiation overhead</li> </ul> <p>Finding model names: Check the providers page for provider IDs, or use the <code>list_models</code> API to see available models for your provider.</p>"},{"location":"quickstart/#streaming","title":"Streaming","text":"<p>For the providers that support streaming, you can enable it by passing <code>stream=True</code>:</p> <pre><code>output = \"\"\nfor chunk in completion(\n    model=\"mistral-small-latest\",\n    provider=\"mistral\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    stream=True\n):\n    chunk_content = chunk.choices[0].delta.content or \"\"\n    print(chunk_content)\n    output += chunk_content\n</code></pre>"},{"location":"quickstart/#reasoning","title":"Reasoning","text":"<p>For providers that support reasoning, you can request thinking traces alongside the response using <code>reasoning_effort</code>:</p> <pre><code>from any_llm import completion\n\nresponse = completion(\n    model=\"claude-sonnet-4-5-20250929\",\n    provider=\"anthropic\",\n    messages=[{\"role\": \"user\", \"content\": \"How many r's are in strawberry?\"}],\n    reasoning_effort=\"high\",\n)\n\n# Access the model's thinking trace\nif response.choices[0].message.reasoning:\n    print(response.choices[0].message.reasoning.content)\n\n# The final answer\nprint(response.choices[0].message.content)\n</code></pre> <p>Reasoning also works with streaming \u2014 each chunk may include <code>chunk.choices[0].delta.reasoning</code>.</p>"},{"location":"quickstart/#embeddings","title":"Embeddings","text":"<p><code>embedding</code> and <code>aembedding</code> allow you to create vector embeddings from text using the same unified interface across providers.</p> <p>Not all providers support embeddings - check the providers documentation to see which ones do.</p> <pre><code>from any_llm import embedding\n\nresult = embedding(\n    model=\"text-embedding-3-small\",\n    provider=\"openai\",\n    inputs=\"Hello, world!\" # can be either string or list of strings\n)\n\n# Access the embedding vector\nembedding_vector = result.data[0].embedding\nprint(f\"Embedding vector length: {len(embedding_vector)}\")\nprint(f\"Tokens used: {result.usage.total_tokens}\")\n</code></pre>"},{"location":"quickstart/#tools","title":"Tools","text":"<p><code>any-llm</code> supports tool calling for providers that support it. You can pass a list of tools where each tool is either:</p> <ol> <li>Python callable - Functions with proper docstrings and type annotations</li> <li>OpenAI Format tool dict - Already in OpenAI tool format</li> </ol> <pre><code>from any_llm import completion\n\ndef get_weather(location: str, unit: str = \"F\") -&gt; str:\n    \"\"\"Get weather information for a location.\n\n    Args:\n        location: The city or location to get weather for\n        unit: Temperature unit, either 'C' or 'F'\n\n    Returns:\n        Current weather description\n    \"\"\"\n    return f\"Weather in {location} is sunny and 75{unit}!\"\n\nresponse = completion(\n    model=\"mistral-small-latest\",\n    provider=\"mistral\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Pittsburgh PA?\"}],\n    tools=[get_weather]\n)\n</code></pre> <p>any-llm automatically converts your Python functions to OpenAI tools format. Functions must have: - A docstring describing what the function does - Type annotations for all parameters - A return type annotation</p>"},{"location":"quickstart/#exception-handling","title":"Exception Handling","text":"<p>The <code>any-llm</code> package provides a unified exception hierarchy that works consistently across all LLM providers.</p>"},{"location":"quickstart/#enabling-unified-exceptions","title":"Enabling Unified Exceptions","text":"<p>Opt-in Feature</p> <p>Unified exception handling is currently opt-in. Set the <code>ANY_LLM_UNIFIED_EXCEPTIONS</code> environment variable to enable it:</p> <pre><code>export ANY_LLM_UNIFIED_EXCEPTIONS=1\n</code></pre> <p>When enabled, provider-specific exceptions are automatically converted to <code>any-llm</code> exception types. When disabled (default), the original provider exceptions are raised with a deprecation warning.</p>"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":"<pre><code>from any_llm import completion\nfrom any_llm.exceptions import (\n    RateLimitError,\n    AuthenticationError,\n    ProviderError,\n    AnyLLMError,\n)\n\ntry:\n    response = completion(\n        model=\"gpt-4\",\n        provider=\"openai\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n    )\nexcept RateLimitError as e:\n    print(f\"Rate limited: {e.message}\")\nexcept AuthenticationError as e:\n    print(f\"Auth failed: {e.message}\")\nexcept ProviderError as e:\n    print(f\"Provider error: {e.message}\")\nexcept AnyLLMError as e:\n    print(f\"Error: {e.message}\")\n</code></pre>"},{"location":"quickstart/#accessing-original-exceptions","title":"Accessing Original Exceptions","text":"<p>All unified exceptions preserve the original provider exception for debugging:</p> <pre><code>from any_llm.exceptions import RateLimitError\n\nmessages = [{\"role\": \"user\", \"content\": \"Hello!\"}]\n\ntry:\n    response = completion(model=\"gpt-4\", provider=\"openai\", messages=messages)\nexcept RateLimitError as e:\n    print(f\"Provider: {e.provider_name}\")\n    print(f\"Original exception: {type(e.original_exception)}\")\n</code></pre>"},{"location":"api/any_llm/","title":"AnyLLM","text":""},{"location":"api/any_llm/#anyllm","title":"AnyLLM","text":""},{"location":"api/any_llm/#any_llm.AnyLLM","title":"<code>any_llm.AnyLLM</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Provider for the LLM.</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>class AnyLLM(ABC):\n    \"\"\"Provider for the LLM.\"\"\"\n\n    # === Provider-specific configuration (to be overridden by subclasses) ===\n    PROVIDER_NAME: str\n    \"\"\"Must match the name of the provider directory  (case sensitive)\"\"\"\n\n    PROVIDER_DOCUMENTATION_URL: str\n    \"\"\"Link to the provider's documentation\"\"\"\n\n    ENV_API_KEY_NAME: str\n    \"\"\"Environment variable name for the API key\"\"\"\n\n    ENV_API_BASE_NAME: str | None = None\n    \"\"\"Environment variable name for the API base URL. Optional.\"\"\"\n\n    # === Feature support flags (to be set by subclasses) ===\n    SUPPORTS_COMPLETION_STREAMING: bool\n    \"\"\"OpenAI Streaming Completion API\"\"\"\n\n    SUPPORTS_COMPLETION: bool\n    \"\"\"OpenAI Completion API\"\"\"\n\n    SUPPORTS_COMPLETION_REASONING: bool\n    \"\"\"Reasoning Content attached to Completion API Response\"\"\"\n\n    SUPPORTS_COMPLETION_IMAGE: bool\n    \"\"\"Image Support for Completion API\"\"\"\n\n    SUPPORTS_COMPLETION_PDF: bool\n    \"\"\"PDF Support for Completion API\"\"\"\n\n    SUPPORTS_EMBEDDING: bool\n    \"\"\"OpenAI Embedding API\"\"\"\n\n    SUPPORTS_RESPONSES: bool\n    \"\"\"OpenAI Responses API\"\"\"\n\n    SUPPORTS_LIST_MODELS: bool\n    \"\"\"OpenAI Models API\"\"\"\n\n    SUPPORTS_BATCH: bool\n    \"\"\"OpenAI Batch Completion API\"\"\"\n\n    API_BASE: str | None = None\n    \"\"\"This is used to set the API base for the provider.\n    It is not required but may prove useful for providers that have overridable api bases.\n    \"\"\"\n\n    # === Internal Flag Checks ===\n    MISSING_PACKAGES_ERROR: ImportError | None = None\n    \"\"\"Some providers use SDKs that are not installed by default.\n    This flag is used to check if the packages are installed before instantiating the provider.\n    \"\"\"\n\n    BUILT_IN_TOOLS: ClassVar[list[Any] | None] = None\n    \"\"\"Some providers have built-in tools that can be used as-is without conversion.\n    This should be a list of the allowed built-in tool instances.\n    For example, in `gemini` provider, this could include `google.genai.types.Tool`.\n    \"\"\"\n\n    ANY_LLM_KEY: str = \"ANY_LLM_KEY\"\n\n    def __init__(self, api_key: str | None = None, api_base: str | None = None, **kwargs: Any) -&gt; None:\n        self._verify_no_missing_packages()\n        self._init_client(\n            api_key=self._verify_and_set_api_key(api_key),\n            api_base=self._resolve_api_base(api_base),\n            **kwargs,\n        )\n\n    def _verify_no_missing_packages(self) -&gt; None:\n        if self.MISSING_PACKAGES_ERROR is not None:\n            msg = f\"{self.PROVIDER_NAME} required packages are not installed. Please install them with `pip install any-llm-sdk[{self.PROVIDER_NAME}]. Specific error message: {self.MISSING_PACKAGES_ERROR.msg}`\"\n            raise ImportError(msg) from self.MISSING_PACKAGES_ERROR\n\n    def _verify_and_set_api_key(self, api_key: str | None = None) -&gt; str | None:\n        # Standardized API key handling. Splitting into its own function so that providers\n        # can easily override this method if they don't want verification (for instance, LMStudio)\n        if not api_key:\n            api_key = os.getenv(self.ENV_API_KEY_NAME)\n\n        if not api_key:\n            raise MissingApiKeyError(self.PROVIDER_NAME, self.ENV_API_KEY_NAME)\n        return api_key\n\n    def _resolve_api_base(self, api_base: str | None = None) -&gt; str | None:\n        \"\"\"Resolve API base URL from parameter or environment variable.\n\n        Resolution order:\n        1. Explicit api_base parameter (if provided)\n        2. Environment variable (if ENV_API_BASE_NAME is defined and set)\n        3. None (allowing _init_client to use API_BASE class default)\n        \"\"\"\n        if api_base:\n            return api_base\n        if self.ENV_API_BASE_NAME:\n            return os.getenv(self.ENV_API_BASE_NAME)\n        return None\n\n    @classmethod\n    def create(\n        cls, provider: str | LLMProvider, api_key: str | None = None, api_base: str | None = None, **kwargs: Any\n    ) -&gt; AnyLLM:\n        \"\"\"Create a provider instance using the given provider name and config.\n\n        Args:\n            provider: The provider name (e.g., 'openai', 'anthropic')\n            api_key: API key for the provider\n            api_base: Base URL for the provider API\n            **kwargs: Additional provider-specific arguments\n\n        Returns:\n            Provider instance for the specified provider\n\n        \"\"\"\n        return cls._create_provider(provider, api_key=api_key, api_base=api_base, **kwargs)\n\n    @classmethod\n    def _create_provider(\n        cls, provider_key: str | LLMProvider, api_key: str | None = None, api_base: str | None = None, **kwargs: Any\n    ) -&gt; AnyLLM:\n        \"\"\"Dynamically load and create an instance of a provider based on the naming convention.\"\"\"\n        provider_key = LLMProvider.from_string(provider_key).value\n\n        provider_class_name = f\"{provider_key.capitalize()}Provider\"\n        provider_module_name = f\"{provider_key}\"\n\n        module_path = f\"any_llm.providers.{provider_module_name}\"\n\n        try:\n            module = importlib.import_module(module_path)\n        except ImportError as e:\n            msg = f\"Could not import module {module_path}: {e!s}. Please ensure the provider is supported by doing AnyLLM.get_supported_providers()\"\n            raise ImportError(msg) from e\n\n        provider_class: type[AnyLLM] = getattr(module, provider_class_name)\n\n        if not api_key:\n            api_key = os.getenv(cls.ANY_LLM_KEY)\n\n        if api_key:\n            try:\n                # Validate if the key conforms with the any-api format.\n                # If it does, any-llm must ask any-api for the corresponding provider key.\n                PlatformKey(api_key=api_key)\n\n                # Import and instantiate PlatformProvider in-place to avoid circular dependency issues.\n                platform_class_name = \"PlatformProvider\"\n                platform_module_path = \"any_llm.providers.platform\"\n                try:\n                    platform_module = importlib.import_module(platform_module_path)\n                except ImportError as e:\n                    msg = f\"Could not import module {module_path}: {e!s}. Please ensure the provider is supported by doing AnyLLM.get_supported_providers()\"\n                    raise ImportError(msg) from e\n\n                platform_class: type[AnyLLM] = getattr(platform_module, platform_class_name)\n\n                # Instantiate the class first and pass the provider next,\n                # so we don't change the common API between different provideers.\n                # pop platform-specific kwargs to avoid passing them to the provider's __init__\n                client_name = kwargs.pop(\"client_name\", None)\n                platform_provider = platform_class(\n                    api_key=api_key, api_base=api_base, client_name=client_name, **kwargs\n                )\n                platform_provider.provider = provider_class  # type: ignore[attr-defined]\n            except ValueError:\n                pass\n            else:\n                return platform_provider\n\n        return provider_class(api_key=api_key, api_base=api_base, **kwargs)\n\n    @classmethod\n    def get_provider_class(cls, provider_key: str | LLMProvider) -&gt; type[AnyLLM]:\n        \"\"\"Get the provider class without instantiating it.\n\n        Args:\n            provider_key: The provider key (e.g., 'anthropic', 'openai')\n\n        Returns:\n            The provider class\n\n        \"\"\"\n        provider_key = LLMProvider.from_string(provider_key).value\n\n        provider_class_name = f\"{provider_key.capitalize()}Provider\"\n        provider_module_name = f\"{provider_key}\"\n\n        module_path = f\"any_llm.providers.{provider_module_name}\"\n\n        try:\n            module = importlib.import_module(module_path)\n        except ImportError as e:\n            msg = f\"Could not import module {module_path}: {e!s}. Please ensure the provider is supported by doing AnyLLM.get_supported_providers()\"\n            raise ImportError(msg) from e\n\n        provider_class: type[AnyLLM] = getattr(module, provider_class_name)\n        return provider_class\n\n    @classmethod\n    def get_supported_providers(cls) -&gt; list[str]:\n        \"\"\"Get a list of supported provider keys.\"\"\"\n        return [provider.value for provider in LLMProvider]\n\n    @classmethod\n    def get_all_provider_metadata(cls) -&gt; list[ProviderMetadata]:\n        \"\"\"Get metadata for all supported providers.\n\n        Returns:\n            List of dictionaries containing provider metadata\n\n        \"\"\"\n        providers: list[ProviderMetadata] = []\n        for provider_key in cls.get_supported_providers():\n            provider_class = cls.get_provider_class(provider_key)\n            metadata = provider_class.get_provider_metadata()\n            providers.append(metadata)\n\n        # Sort providers by name\n        providers.sort(key=lambda x: x.name)\n        return providers\n\n    @classmethod\n    def get_provider_enum(cls, provider_key: str) -&gt; LLMProvider:\n        \"\"\"Convert a string provider key to a ProviderName enum.\"\"\"\n        try:\n            return LLMProvider(provider_key)\n        except ValueError as e:\n            supported = [provider.value for provider in LLMProvider]\n            raise UnsupportedProviderError(provider_key, supported) from e\n\n    @classmethod\n    def split_model_provider(cls, model: str) -&gt; tuple[LLMProvider, str]:\n        \"\"\"Extract the provider key from the model identifier.\n\n        Supports both new format 'provider:model' (e.g., 'mistral:mistral-small')\n        and legacy format 'provider/model' (e.g., 'mistral/mistral-small').\n\n        The legacy format will be deprecated in version 1.0.\n        \"\"\"\n        colon_index = model.find(\":\")\n        slash_index = model.find(\"/\")\n\n        # Determine which delimiter comes first\n        if colon_index != -1 and (slash_index == -1 or colon_index &lt; slash_index):\n            # The colon came first, so it's using the new syntax.\n            provider, model_name = model.split(\":\", 1)\n        elif slash_index != -1:\n            # Slash comes first, so it's the legacy syntax\n            warnings.warn(\n                f\"Model format 'provider/model' is deprecated and will be removed in version 1.0. \"\n                f\"Please use 'provider:model' format instead. Got: '{model}'\",\n                DeprecationWarning,\n                stacklevel=3,\n            )\n            provider, model_name = model.split(\"/\", 1)\n        else:\n            msg = f\"Invalid model format. Expected 'provider:model' or 'provider/model', got '{model}'\"\n            raise ValueError(msg)\n\n        if not provider or not model_name:\n            msg = f\"Invalid model format. Expected 'provider:model' or 'provider/model', got '{model}'\"\n            raise ValueError(msg)\n        return cls.get_provider_enum(provider), model_name\n\n    @staticmethod\n    @abstractmethod\n    def _convert_completion_params(params: CompletionParams, **kwargs: Any) -&gt; dict[str, Any]:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    @staticmethod\n    @abstractmethod\n    def _convert_completion_response(response: Any) -&gt; ChatCompletion:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    @staticmethod\n    @abstractmethod\n    def _convert_completion_chunk_response(response: Any, **kwargs: Any) -&gt; ChatCompletionChunk:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    @staticmethod\n    @abstractmethod\n    def _convert_embedding_params(params: Any, **kwargs: Any) -&gt; dict[str, Any]:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    @staticmethod\n    @abstractmethod\n    def _convert_embedding_response(response: Any) -&gt; CreateEmbeddingResponse:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    @staticmethod\n    @abstractmethod\n    def _convert_list_models_response(response: Any) -&gt; Sequence[Model]:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    @classmethod\n    def get_provider_metadata(cls) -&gt; ProviderMetadata:\n        \"\"\"Get provider metadata without requiring instantiation.\n\n        Returns:\n            Dictionary containing provider metadata including name, environment variable,\n            documentation URL, and class name.\n\n        \"\"\"\n        return ProviderMetadata(\n            name=cls.PROVIDER_NAME,\n            env_key=cls.ENV_API_KEY_NAME,\n            env_api_base=cls.ENV_API_BASE_NAME,\n            doc_url=cls.PROVIDER_DOCUMENTATION_URL,\n            streaming=cls.SUPPORTS_COMPLETION_STREAMING,\n            reasoning=cls.SUPPORTS_COMPLETION_REASONING,\n            completion=cls.SUPPORTS_COMPLETION,\n            image=cls.SUPPORTS_COMPLETION_IMAGE,\n            pdf=cls.SUPPORTS_COMPLETION_PDF,\n            embedding=cls.SUPPORTS_EMBEDDING,\n            responses=cls.SUPPORTS_RESPONSES,\n            list_models=cls.SUPPORTS_LIST_MODELS,\n            batch_completion=cls.SUPPORTS_BATCH,\n            class_name=cls.__name__,\n        )\n\n    @abstractmethod\n    def _init_client(self, api_key: str | None = None, api_base: str | None = None, **kwargs: Any) -&gt; None:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    def completion(\n        self,\n        **kwargs: Any,\n    ) -&gt; ChatCompletion | Iterator[ChatCompletionChunk]:\n        \"\"\"Create a chat completion synchronously.\n\n        See [AnyLLM.acompletion][any_llm.any_llm.AnyLLM.acompletion]\n        \"\"\"\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        response = run_async_in_sync(self.acompletion(**kwargs), allow_running_loop=allow_running_loop)\n        if isinstance(response, ChatCompletion):\n            return response\n\n        return async_iter_to_sync_iter(response)\n\n    @handle_exceptions(wrap_streaming=True)\n    async def acompletion(\n        self,\n        model: str,\n        messages: list[dict[str, Any] | ChatCompletionMessage],\n        *,\n        tools: list[dict[str, Any] | Callable[..., Any]] | Any | None = None,\n        tool_choice: str | dict[str, Any] | None = None,\n        temperature: float | None = None,\n        top_p: float | None = None,\n        max_tokens: int | None = None,\n        response_format: dict[str, Any] | type[BaseModel] | None = None,\n        stream: bool | None = None,\n        n: int | None = None,\n        stop: str | list[str] | None = None,\n        presence_penalty: float | None = None,\n        frequency_penalty: float | None = None,\n        seed: int | None = None,\n        user: str | None = None,\n        parallel_tool_calls: bool | None = None,\n        logprobs: bool | None = None,\n        top_logprobs: int | None = None,\n        logit_bias: dict[str, float] | None = None,\n        stream_options: dict[str, Any] | None = None,\n        max_completion_tokens: int | None = None,\n        reasoning_effort: ReasoningEffort | None = \"auto\",\n        **kwargs: Any,\n    ) -&gt; ChatCompletion | AsyncIterator[ChatCompletionChunk]:\n        \"\"\"Create a chat completion asynchronously.\n\n        Args:\n            model: Model identifier for the chosen provider (e.g., model='gpt-4.1-mini' for LLMProvider.OPENAI).\n            messages: List of messages for the conversation\n            tools: List of tools for tool calling. Can be Python callables or OpenAI tool format dicts\n            tool_choice: Controls which tools the model can call\n            temperature: Controls randomness in the response (0.0 to 2.0)\n            top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n            max_tokens: Maximum number of tokens to generate\n            response_format: Format specification for the response\n            stream: Whether to stream the response\n            n: Number of completions to generate\n            stop: Stop sequences for generation\n            presence_penalty: Penalize new tokens based on presence in text\n            frequency_penalty: Penalize new tokens based on frequency in text\n            seed: Random seed for reproducible results\n            user: Unique identifier for the end user\n            parallel_tool_calls: Whether to allow parallel tool calls\n            logprobs: Include token-level log probabilities in the response\n            top_logprobs: Number of alternatives to return when logprobs are requested\n            logit_bias: Bias the likelihood of specified tokens during generation\n            stream_options: Additional options controlling streaming behavior\n            max_completion_tokens: Maximum number of tokens for the completion\n            reasoning_effort: Reasoning effort level for models that support it. \"auto\" will map to each provider's default.\n            **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n        Returns:\n            The completion response from the provider\n\n        \"\"\"\n        prepared_tools = None\n        if tools:\n            prepared_tools = prepare_tools(tools, built_in_tools=self.BUILT_IN_TOOLS)\n\n        processed_messages: list[dict[str, Any]] = []\n        for message in messages:\n            if isinstance(message, ChatCompletionMessage):\n                # Dump the message but exclude the extra field that we extend from OpenAI Spec\n                processed_messages.append(message.model_dump(exclude_none=True, exclude={\"reasoning\"}))\n            else:\n                processed_messages.append(message)\n\n        params = CompletionParams(\n            model_id=model,\n            messages=processed_messages,\n            tools=prepared_tools,\n            tool_choice=tool_choice,\n            temperature=temperature,\n            top_p=top_p,\n            max_tokens=max_tokens,\n            response_format=response_format,\n            stream=stream,\n            n=n,\n            stop=stop,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            seed=seed,\n            user=user,\n            parallel_tool_calls=parallel_tool_calls,\n            logprobs=logprobs,\n            top_logprobs=top_logprobs,\n            logit_bias=logit_bias,\n            stream_options=stream_options,\n            max_completion_tokens=max_completion_tokens,\n            reasoning_effort=reasoning_effort,\n        )\n\n        return await self._acompletion(params, **kwargs)\n\n    async def _acompletion(\n        self, params: CompletionParams, **kwargs: Any\n    ) -&gt; ChatCompletion | AsyncIterator[ChatCompletionChunk]:\n        if not self.SUPPORTS_COMPLETION:\n            msg = \"Provider doesn't support completion.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _acompletion method\"\n        raise NotImplementedError(msg)\n\n    def responses(self, **kwargs: Any) -&gt; ResponseResource | Response | Iterator[ResponseStreamEvent]:\n        \"\"\"Create a response synchronously.\n\n        See [AnyLLM.aresponses][any_llm.any_llm.AnyLLM.aresponses]\n        \"\"\"\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        response = run_async_in_sync(self.aresponses(**kwargs), allow_running_loop=allow_running_loop)\n        if isinstance(response, (ResponseResource, Response)):\n            return response\n        return async_iter_to_sync_iter(response)\n\n    @handle_exceptions(wrap_streaming=True)\n    async def aresponses(\n        self,\n        model: str,\n        input_data: str | ResponseInputParam,\n        *,\n        tools: list[dict[str, Any] | Callable[..., Any]] | Any | None = None,\n        tool_choice: str | dict[str, Any] | None = None,\n        max_output_tokens: int | None = None,\n        temperature: float | None = None,\n        top_p: float | None = None,\n        stream: bool | None = None,\n        instructions: str | None = None,\n        max_tool_calls: int | None = None,\n        parallel_tool_calls: int | None = None,\n        reasoning: Any | None = None,\n        text: Any | None = None,\n        presence_penalty: float | None = None,\n        frequency_penalty: float | None = None,\n        truncation: str | None = None,\n        store: bool | None = None,\n        service_tier: str | None = None,\n        user: str | None = None,\n        metadata: dict[str, str] | None = None,\n        previous_response_id: str | None = None,\n        include: list[str] | None = None,\n        background: bool | None = None,\n        safety_identifier: str | None = None,\n        prompt_cache_key: str | None = None,\n        prompt_cache_retention: str | None = None,\n        conversation: str | dict[str, Any] | None = None,\n        **kwargs: Any,\n    ) -&gt; ResponseResource | Response | AsyncIterator[ResponseStreamEvent]:\n        \"\"\"Create a response using the OpenResponses API.\n\n        This implements the OpenResponses specification and returns either\n        `openresponses_types.ResponseResource` (for OpenResponses-compliant providers)\n        or `openai.types.responses.Response` (for providers using OpenAI's native API).\n        If `stream=True`, an iterator of streaming event dicts is returned.\n\n        Args:\n            model: Model identifier for the chosen provider (e.g., model='gpt-4.1-mini' for LLMProvider.OPENAI).\n            input_data: The input payload accepted by provider's Responses API.\n                For OpenAI-compatible providers, this is typically a list mixing\n                text, images, and tool instructions, or a dict per OpenAI spec.\n            tools: Optional tools for tool calling (Python callables or OpenAI tool dicts)\n            tool_choice: Controls which tools the model can call\n            max_output_tokens: Maximum number of output tokens to generate\n            temperature: Controls randomness in the response (0.0 to 2.0)\n            top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n            stream: Whether to stream response events\n            instructions: A system (or developer) message inserted into the model's context.\n            max_tool_calls: The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.\n            parallel_tool_calls: Whether to allow the model to run tool calls in parallel.\n            reasoning: Configuration options for reasoning models.\n            text: Configuration options for a text response from the model. Can be plain text or structured JSON data.\n            presence_penalty: Penalizes new tokens based on whether they appear in the text so far.\n            frequency_penalty: Penalizes new tokens based on their frequency in the text so far.\n            truncation: Controls how the service truncates input when it exceeds the model context window.\n            store: Whether to store the response so it can be retrieved later.\n            service_tier: The service tier to use for this request.\n            user: A unique identifier representing your end user.\n            metadata: Key-value pairs for custom metadata (up to 16 pairs).\n            previous_response_id: The ID of the response to use as the prior turn for this request.\n            include: Items to include in the response (e.g., 'reasoning.encrypted_content').\n            background: Whether to run the request in the background and return immediately.\n            safety_identifier: A stable identifier used for safety monitoring and abuse detection.\n            prompt_cache_key: A key to use when reading from or writing to the prompt cache.\n            prompt_cache_retention: How long to retain a prompt cache entry created by this request.\n            conversation: The conversation to associate this response with (ID string or ConversationParam object).\n            **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n        Returns:\n            Either a `ResponseResource` object (OpenResponses-compliant providers),\n            a `Response` object (non-compliant providers), or an iterator of\n            `ResponseStreamEvent` (streaming).\n\n        Raises:\n            NotImplementedError: If the selected provider does not support the Responses API.\n\n        \"\"\"\n        prepared_tools = None\n        if tools:\n            prepared_tools = prepare_tools(tools, built_in_tools=self.BUILT_IN_TOOLS)\n\n        params = ResponsesParams(\n            model=model,\n            input=input_data,\n            tools=prepared_tools,\n            tool_choice=tool_choice,\n            max_output_tokens=max_output_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            stream=stream,\n            instructions=instructions,\n            max_tool_calls=max_tool_calls,\n            parallel_tool_calls=bool(parallel_tool_calls),\n            reasoning=reasoning,\n            text=text,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            truncation=truncation,\n            store=store,\n            service_tier=service_tier,\n            user=user,\n            metadata=metadata,\n            previous_response_id=previous_response_id,\n            include=include,\n            background=background,\n            safety_identifier=safety_identifier,\n            prompt_cache_key=prompt_cache_key,\n            prompt_cache_retention=prompt_cache_retention,\n            conversation=conversation,\n            **kwargs,\n        )\n\n        return await self._aresponses(params)\n\n    async def _aresponses(\n        self, params: ResponsesParams, **kwargs: Any\n    ) -&gt; ResponseResource | Response | AsyncIterator[ResponseStreamEvent]:\n        if not self.SUPPORTS_RESPONSES:\n            msg = \"Provider doesn't support responses.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _aresponses method\"\n        raise NotImplementedError(msg)\n\n    def _embedding(self, model: str, inputs: str | list[str], **kwargs: Any) -&gt; CreateEmbeddingResponse:\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        return run_async_in_sync(self.aembedding(model, inputs, **kwargs), allow_running_loop=allow_running_loop)\n\n    @handle_exceptions()\n    async def aembedding(self, model: str, inputs: str | list[str], **kwargs: Any) -&gt; CreateEmbeddingResponse:\n        return await self._aembedding(model, inputs, **kwargs)\n\n    async def _aembedding(self, model: str, inputs: str | list[str], **kwargs: Any) -&gt; CreateEmbeddingResponse:\n        if not self.SUPPORTS_EMBEDDING:\n            msg = \"Provider doesn't support embedding.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _aembedding method\"\n        raise NotImplementedError(msg)\n\n    def list_models(self, **kwargs: Any) -&gt; Sequence[Model]:\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        return run_async_in_sync(self.alist_models(**kwargs), allow_running_loop=allow_running_loop)\n\n    @handle_exceptions()\n    async def alist_models(self, **kwargs: Any) -&gt; Sequence[Model]:\n        return await self._alist_models(**kwargs)\n\n    async def _alist_models(self, **kwargs: Any) -&gt; Sequence[Model]:\n        if not self.SUPPORTS_LIST_MODELS:\n            msg = \"Provider doesn't support listing models.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _alist_models method\"\n        raise NotImplementedError(msg)\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    def create_batch(self, **kwargs: Any) -&gt; Batch:\n        \"\"\"Create a batch synchronously.\n\n        See [AnyLLM.acreate_batch][any_llm.any_llm.AnyLLM.acreate_batch]\n        \"\"\"\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        return run_async_in_sync(self.acreate_batch(**kwargs), allow_running_loop=allow_running_loop)\n\n    @handle_exceptions()\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    async def acreate_batch(\n        self,\n        input_file_path: str,\n        endpoint: str,\n        completion_window: str = \"24h\",\n        metadata: dict[str, str] | None = None,\n        **kwargs: Any,\n    ) -&gt; Batch:\n        \"\"\"Create a batch job asynchronously.\n\n        Args:\n            input_file_path: Path to a local file containing batch requests in JSONL format.\n            endpoint: The endpoint to be used for all requests (e.g., '/v1/chat/completions')\n            completion_window: The time frame within which the batch should be processed (default: '24h')\n            metadata: Optional custom metadata for the batch\n            **kwargs: Additional provider-specific arguments\n\n        Returns:\n            The created batch object\n\n        \"\"\"\n        return await self._acreate_batch(\n            input_file_path=input_file_path,\n            endpoint=endpoint,\n            completion_window=completion_window,\n            metadata=metadata,\n            **kwargs,\n        )\n\n    async def _acreate_batch(\n        self,\n        input_file_path: str,\n        endpoint: str,\n        completion_window: str = \"24h\",\n        metadata: dict[str, str] | None = None,\n        **kwargs: Any,\n    ) -&gt; Batch:\n        if not self.SUPPORTS_BATCH:\n            msg = \"Provider doesn't support batch completions.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _acreate_batch method\"\n        raise NotImplementedError(msg)\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    def retrieve_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n        \"\"\"Retrieve a batch synchronously.\n\n        See [AnyLLM.aretrieve_batch][any_llm.any_llm.AnyLLM.aretrieve_batch]\n        \"\"\"\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        return run_async_in_sync(self.aretrieve_batch(batch_id, **kwargs), allow_running_loop=allow_running_loop)\n\n    @handle_exceptions()\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    async def aretrieve_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n        \"\"\"Retrieve a batch job asynchronously.\n\n        Args:\n            batch_id: The ID of the batch to retrieve\n            **kwargs: Additional provider-specific arguments\n\n        Returns:\n            The batch object\n\n        \"\"\"\n        return await self._aretrieve_batch(batch_id, **kwargs)\n\n    async def _aretrieve_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n        if not self.SUPPORTS_BATCH:\n            msg = \"Provider doesn't support batch completions.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _aretrieve_batch method\"\n        raise NotImplementedError(msg)\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    def cancel_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n        \"\"\"Cancel a batch synchronously.\n\n        See [AnyLLM.acancel_batch][any_llm.any_llm.AnyLLM.acancel_batch]\n        \"\"\"\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        return run_async_in_sync(self.acancel_batch(batch_id, **kwargs), allow_running_loop=allow_running_loop)\n\n    @handle_exceptions()\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    async def acancel_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n        \"\"\"Cancel a batch job asynchronously.\n\n        Args:\n            batch_id: The ID of the batch to cancel\n            **kwargs: Additional provider-specific arguments\n\n        Returns:\n            The cancelled batch object\n\n        \"\"\"\n        return await self._acancel_batch(batch_id, **kwargs)\n\n    async def _acancel_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n        if not self.SUPPORTS_BATCH:\n            msg = \"Provider doesn't support batch completions.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _acancel_batch method\"\n        raise NotImplementedError(msg)\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    def list_batches(\n        self,\n        after: str | None = None,\n        limit: int | None = None,\n        **kwargs: Any,\n    ) -&gt; Sequence[Batch]:\n        \"\"\"List batches synchronously.\n\n        See [AnyLLM.alist_batches][any_llm.any_llm.AnyLLM.alist_batches]\n        \"\"\"\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        return run_async_in_sync(\n            self.alist_batches(after=after, limit=limit, **kwargs), allow_running_loop=allow_running_loop\n        )\n\n    @handle_exceptions()\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    async def alist_batches(\n        self,\n        after: str | None = None,\n        limit: int | None = None,\n        **kwargs: Any,\n    ) -&gt; Sequence[Batch]:\n        \"\"\"List batch jobs asynchronously.\n\n        Args:\n            after: A cursor for pagination. Returns batches after this batch ID.\n            limit: Maximum number of batches to return (default: 20)\n            **kwargs: Additional provider-specific arguments\n\n        Returns:\n            A list of batch objects\n\n        \"\"\"\n        return await self._alist_batches(after=after, limit=limit, **kwargs)\n\n    async def _alist_batches(\n        self,\n        after: str | None = None,\n        limit: int | None = None,\n        **kwargs: Any,\n    ) -&gt; Sequence[Batch]:\n        if not self.SUPPORTS_BATCH:\n            msg = \"Provider doesn't support batch completions.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _alist_batches method\"\n        raise NotImplementedError(msg)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.API_BASE","title":"<code>API_BASE = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>This is used to set the API base for the provider. It is not required but may prove useful for providers that have overridable api bases.</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.BUILT_IN_TOOLS","title":"<code>BUILT_IN_TOOLS = None</code>  <code>class-attribute</code>","text":"<p>Some providers have built-in tools that can be used as-is without conversion. This should be a list of the allowed built-in tool instances. For example, in <code>gemini</code> provider, this could include <code>google.genai.types.Tool</code>.</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.ENV_API_BASE_NAME","title":"<code>ENV_API_BASE_NAME = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Environment variable name for the API base URL. Optional.</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.ENV_API_KEY_NAME","title":"<code>ENV_API_KEY_NAME</code>  <code>instance-attribute</code>","text":"<p>Environment variable name for the API key</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.MISSING_PACKAGES_ERROR","title":"<code>MISSING_PACKAGES_ERROR = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Some providers use SDKs that are not installed by default. This flag is used to check if the packages are installed before instantiating the provider.</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.PROVIDER_DOCUMENTATION_URL","title":"<code>PROVIDER_DOCUMENTATION_URL</code>  <code>instance-attribute</code>","text":"<p>Link to the provider's documentation</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.PROVIDER_NAME","title":"<code>PROVIDER_NAME</code>  <code>instance-attribute</code>","text":"<p>Must match the name of the provider directory  (case sensitive)</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_BATCH","title":"<code>SUPPORTS_BATCH</code>  <code>instance-attribute</code>","text":"<p>OpenAI Batch Completion API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_COMPLETION","title":"<code>SUPPORTS_COMPLETION</code>  <code>instance-attribute</code>","text":"<p>OpenAI Completion API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_COMPLETION_IMAGE","title":"<code>SUPPORTS_COMPLETION_IMAGE</code>  <code>instance-attribute</code>","text":"<p>Image Support for Completion API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_COMPLETION_PDF","title":"<code>SUPPORTS_COMPLETION_PDF</code>  <code>instance-attribute</code>","text":"<p>PDF Support for Completion API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_COMPLETION_REASONING","title":"<code>SUPPORTS_COMPLETION_REASONING</code>  <code>instance-attribute</code>","text":"<p>Reasoning Content attached to Completion API Response</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_COMPLETION_STREAMING","title":"<code>SUPPORTS_COMPLETION_STREAMING</code>  <code>instance-attribute</code>","text":"<p>OpenAI Streaming Completion API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_EMBEDDING","title":"<code>SUPPORTS_EMBEDDING</code>  <code>instance-attribute</code>","text":"<p>OpenAI Embedding API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_LIST_MODELS","title":"<code>SUPPORTS_LIST_MODELS</code>  <code>instance-attribute</code>","text":"<p>OpenAI Models API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_RESPONSES","title":"<code>SUPPORTS_RESPONSES</code>  <code>instance-attribute</code>","text":"<p>OpenAI Responses API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.acancel_batch","title":"<code>acancel_batch(batch_id, **kwargs)</code>  <code>async</code>","text":"<p>Cancel a batch job asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>batch_id</code> <code>str</code> <p>The ID of the batch to cancel</p> required <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The cancelled batch object</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@handle_exceptions()\n@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def acancel_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n    \"\"\"Cancel a batch job asynchronously.\n\n    Args:\n        batch_id: The ID of the batch to cancel\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The cancelled batch object\n\n    \"\"\"\n    return await self._acancel_batch(batch_id, **kwargs)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.acompletion","title":"<code>acompletion(model, messages, *, tools=None, tool_choice=None, temperature=None, top_p=None, max_tokens=None, response_format=None, stream=None, n=None, stop=None, presence_penalty=None, frequency_penalty=None, seed=None, user=None, parallel_tool_calls=None, logprobs=None, top_logprobs=None, logit_bias=None, stream_options=None, max_completion_tokens=None, reasoning_effort='auto', **kwargs)</code>  <code>async</code>","text":"<p>Create a chat completion asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier for the chosen provider (e.g., model='gpt-4.1-mini' for LLMProvider.OPENAI).</p> required <code>messages</code> <code>list[dict[str, Any] | ChatCompletionMessage]</code> <p>List of messages for the conversation</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | Any | None</code> <p>List of tools for tool calling. Can be Python callables or OpenAI tool format dicts</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>max_tokens</code> <code>int | None</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>response_format</code> <code>dict[str, Any] | type[BaseModel] | None</code> <p>Format specification for the response</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream the response</p> <code>None</code> <code>n</code> <code>int | None</code> <p>Number of completions to generate</p> <code>None</code> <code>stop</code> <code>str | list[str] | None</code> <p>Stop sequences for generation</p> <code>None</code> <code>presence_penalty</code> <code>float | None</code> <p>Penalize new tokens based on presence in text</p> <code>None</code> <code>frequency_penalty</code> <code>float | None</code> <p>Penalize new tokens based on frequency in text</p> <code>None</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducible results</p> <code>None</code> <code>user</code> <code>str | None</code> <p>Unique identifier for the end user</p> <code>None</code> <code>parallel_tool_calls</code> <code>bool | None</code> <p>Whether to allow parallel tool calls</p> <code>None</code> <code>logprobs</code> <code>bool | None</code> <p>Include token-level log probabilities in the response</p> <code>None</code> <code>top_logprobs</code> <code>int | None</code> <p>Number of alternatives to return when logprobs are requested</p> <code>None</code> <code>logit_bias</code> <code>dict[str, float] | None</code> <p>Bias the likelihood of specified tokens during generation</p> <code>None</code> <code>stream_options</code> <code>dict[str, Any] | None</code> <p>Additional options controlling streaming behavior</p> <code>None</code> <code>max_completion_tokens</code> <code>int | None</code> <p>Maximum number of tokens for the completion</p> <code>None</code> <code>reasoning_effort</code> <code>ReasoningEffort | None</code> <p>Reasoning effort level for models that support it. \"auto\" will map to each provider's default.</p> <code>'auto'</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatCompletion | AsyncIterator[ChatCompletionChunk]</code> <p>The completion response from the provider</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@handle_exceptions(wrap_streaming=True)\nasync def acompletion(\n    self,\n    model: str,\n    messages: list[dict[str, Any] | ChatCompletionMessage],\n    *,\n    tools: list[dict[str, Any] | Callable[..., Any]] | Any | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    max_tokens: int | None = None,\n    response_format: dict[str, Any] | type[BaseModel] | None = None,\n    stream: bool | None = None,\n    n: int | None = None,\n    stop: str | list[str] | None = None,\n    presence_penalty: float | None = None,\n    frequency_penalty: float | None = None,\n    seed: int | None = None,\n    user: str | None = None,\n    parallel_tool_calls: bool | None = None,\n    logprobs: bool | None = None,\n    top_logprobs: int | None = None,\n    logit_bias: dict[str, float] | None = None,\n    stream_options: dict[str, Any] | None = None,\n    max_completion_tokens: int | None = None,\n    reasoning_effort: ReasoningEffort | None = \"auto\",\n    **kwargs: Any,\n) -&gt; ChatCompletion | AsyncIterator[ChatCompletionChunk]:\n    \"\"\"Create a chat completion asynchronously.\n\n    Args:\n        model: Model identifier for the chosen provider (e.g., model='gpt-4.1-mini' for LLMProvider.OPENAI).\n        messages: List of messages for the conversation\n        tools: List of tools for tool calling. Can be Python callables or OpenAI tool format dicts\n        tool_choice: Controls which tools the model can call\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        max_tokens: Maximum number of tokens to generate\n        response_format: Format specification for the response\n        stream: Whether to stream the response\n        n: Number of completions to generate\n        stop: Stop sequences for generation\n        presence_penalty: Penalize new tokens based on presence in text\n        frequency_penalty: Penalize new tokens based on frequency in text\n        seed: Random seed for reproducible results\n        user: Unique identifier for the end user\n        parallel_tool_calls: Whether to allow parallel tool calls\n        logprobs: Include token-level log probabilities in the response\n        top_logprobs: Number of alternatives to return when logprobs are requested\n        logit_bias: Bias the likelihood of specified tokens during generation\n        stream_options: Additional options controlling streaming behavior\n        max_completion_tokens: Maximum number of tokens for the completion\n        reasoning_effort: Reasoning effort level for models that support it. \"auto\" will map to each provider's default.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        The completion response from the provider\n\n    \"\"\"\n    prepared_tools = None\n    if tools:\n        prepared_tools = prepare_tools(tools, built_in_tools=self.BUILT_IN_TOOLS)\n\n    processed_messages: list[dict[str, Any]] = []\n    for message in messages:\n        if isinstance(message, ChatCompletionMessage):\n            # Dump the message but exclude the extra field that we extend from OpenAI Spec\n            processed_messages.append(message.model_dump(exclude_none=True, exclude={\"reasoning\"}))\n        else:\n            processed_messages.append(message)\n\n    params = CompletionParams(\n        model_id=model,\n        messages=processed_messages,\n        tools=prepared_tools,\n        tool_choice=tool_choice,\n        temperature=temperature,\n        top_p=top_p,\n        max_tokens=max_tokens,\n        response_format=response_format,\n        stream=stream,\n        n=n,\n        stop=stop,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        seed=seed,\n        user=user,\n        parallel_tool_calls=parallel_tool_calls,\n        logprobs=logprobs,\n        top_logprobs=top_logprobs,\n        logit_bias=logit_bias,\n        stream_options=stream_options,\n        max_completion_tokens=max_completion_tokens,\n        reasoning_effort=reasoning_effort,\n    )\n\n    return await self._acompletion(params, **kwargs)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.acreate_batch","title":"<code>acreate_batch(input_file_path, endpoint, completion_window='24h', metadata=None, **kwargs)</code>  <code>async</code>","text":"<p>Create a batch job asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>input_file_path</code> <code>str</code> <p>Path to a local file containing batch requests in JSONL format.</p> required <code>endpoint</code> <code>str</code> <p>The endpoint to be used for all requests (e.g., '/v1/chat/completions')</p> required <code>completion_window</code> <code>str</code> <p>The time frame within which the batch should be processed (default: '24h')</p> <code>'24h'</code> <code>metadata</code> <code>dict[str, str] | None</code> <p>Optional custom metadata for the batch</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The created batch object</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@handle_exceptions()\n@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def acreate_batch(\n    self,\n    input_file_path: str,\n    endpoint: str,\n    completion_window: str = \"24h\",\n    metadata: dict[str, str] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Create a batch job asynchronously.\n\n    Args:\n        input_file_path: Path to a local file containing batch requests in JSONL format.\n        endpoint: The endpoint to be used for all requests (e.g., '/v1/chat/completions')\n        completion_window: The time frame within which the batch should be processed (default: '24h')\n        metadata: Optional custom metadata for the batch\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The created batch object\n\n    \"\"\"\n    return await self._acreate_batch(\n        input_file_path=input_file_path,\n        endpoint=endpoint,\n        completion_window=completion_window,\n        metadata=metadata,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.alist_batches","title":"<code>alist_batches(after=None, limit=None, **kwargs)</code>  <code>async</code>","text":"<p>List batch jobs asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | None</code> <p>A cursor for pagination. Returns batches after this batch ID.</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>Maximum number of batches to return (default: 20)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Sequence[Batch]</code> <p>A list of batch objects</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@handle_exceptions()\n@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def alist_batches(\n    self,\n    after: str | None = None,\n    limit: int | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Batch]:\n    \"\"\"List batch jobs asynchronously.\n\n    Args:\n        after: A cursor for pagination. Returns batches after this batch ID.\n        limit: Maximum number of batches to return (default: 20)\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        A list of batch objects\n\n    \"\"\"\n    return await self._alist_batches(after=after, limit=limit, **kwargs)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.aresponses","title":"<code>aresponses(model, input_data, *, tools=None, tool_choice=None, max_output_tokens=None, temperature=None, top_p=None, stream=None, instructions=None, max_tool_calls=None, parallel_tool_calls=None, reasoning=None, text=None, presence_penalty=None, frequency_penalty=None, truncation=None, store=None, service_tier=None, user=None, metadata=None, previous_response_id=None, include=None, background=None, safety_identifier=None, prompt_cache_key=None, prompt_cache_retention=None, conversation=None, **kwargs)</code>  <code>async</code>","text":"<p>Create a response using the OpenResponses API.</p> <p>This implements the OpenResponses specification and returns either <code>openresponses_types.ResponseResource</code> (for OpenResponses-compliant providers) or <code>openai.types.responses.Response</code> (for providers using OpenAI's native API). If <code>stream=True</code>, an iterator of streaming event dicts is returned.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier for the chosen provider (e.g., model='gpt-4.1-mini' for LLMProvider.OPENAI).</p> required <code>input_data</code> <code>str | ResponseInputParam</code> <p>The input payload accepted by provider's Responses API. For OpenAI-compatible providers, this is typically a list mixing text, images, and tool instructions, or a dict per OpenAI spec.</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | Any | None</code> <p>Optional tools for tool calling (Python callables or OpenAI tool dicts)</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>max_output_tokens</code> <code>int | None</code> <p>Maximum number of output tokens to generate</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream response events</p> <code>None</code> <code>instructions</code> <code>str | None</code> <p>A system (or developer) message inserted into the model's context.</p> <code>None</code> <code>max_tool_calls</code> <code>int | None</code> <p>The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.</p> <code>None</code> <code>parallel_tool_calls</code> <code>int | None</code> <p>Whether to allow the model to run tool calls in parallel.</p> <code>None</code> <code>reasoning</code> <code>Any | None</code> <p>Configuration options for reasoning models.</p> <code>None</code> <code>text</code> <code>Any | None</code> <p>Configuration options for a text response from the model. Can be plain text or structured JSON data.</p> <code>None</code> <code>presence_penalty</code> <code>float | None</code> <p>Penalizes new tokens based on whether they appear in the text so far.</p> <code>None</code> <code>frequency_penalty</code> <code>float | None</code> <p>Penalizes new tokens based on their frequency in the text so far.</p> <code>None</code> <code>truncation</code> <code>str | None</code> <p>Controls how the service truncates input when it exceeds the model context window.</p> <code>None</code> <code>store</code> <code>bool | None</code> <p>Whether to store the response so it can be retrieved later.</p> <code>None</code> <code>service_tier</code> <code>str | None</code> <p>The service tier to use for this request.</p> <code>None</code> <code>user</code> <code>str | None</code> <p>A unique identifier representing your end user.</p> <code>None</code> <code>metadata</code> <code>dict[str, str] | None</code> <p>Key-value pairs for custom metadata (up to 16 pairs).</p> <code>None</code> <code>previous_response_id</code> <code>str | None</code> <p>The ID of the response to use as the prior turn for this request.</p> <code>None</code> <code>include</code> <code>list[str] | None</code> <p>Items to include in the response (e.g., 'reasoning.encrypted_content').</p> <code>None</code> <code>background</code> <code>bool | None</code> <p>Whether to run the request in the background and return immediately.</p> <code>None</code> <code>safety_identifier</code> <code>str | None</code> <p>A stable identifier used for safety monitoring and abuse detection.</p> <code>None</code> <code>prompt_cache_key</code> <code>str | None</code> <p>A key to use when reading from or writing to the prompt cache.</p> <code>None</code> <code>prompt_cache_retention</code> <code>str | None</code> <p>How long to retain a prompt cache entry created by this request.</p> <code>None</code> <code>conversation</code> <code>str | dict[str, Any] | None</code> <p>The conversation to associate this response with (ID string or ConversationParam object).</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ResponseResource | Response | AsyncIterator[ResponseStreamEvent]</code> <p>Either a <code>ResponseResource</code> object (OpenResponses-compliant providers),</p> <code>ResponseResource | Response | AsyncIterator[ResponseStreamEvent]</code> <p>a <code>Response</code> object (non-compliant providers), or an iterator of</p> <code>ResponseResource | Response | AsyncIterator[ResponseStreamEvent]</code> <p><code>ResponseStreamEvent</code> (streaming).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the selected provider does not support the Responses API.</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@handle_exceptions(wrap_streaming=True)\nasync def aresponses(\n    self,\n    model: str,\n    input_data: str | ResponseInputParam,\n    *,\n    tools: list[dict[str, Any] | Callable[..., Any]] | Any | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    max_output_tokens: int | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    stream: bool | None = None,\n    instructions: str | None = None,\n    max_tool_calls: int | None = None,\n    parallel_tool_calls: int | None = None,\n    reasoning: Any | None = None,\n    text: Any | None = None,\n    presence_penalty: float | None = None,\n    frequency_penalty: float | None = None,\n    truncation: str | None = None,\n    store: bool | None = None,\n    service_tier: str | None = None,\n    user: str | None = None,\n    metadata: dict[str, str] | None = None,\n    previous_response_id: str | None = None,\n    include: list[str] | None = None,\n    background: bool | None = None,\n    safety_identifier: str | None = None,\n    prompt_cache_key: str | None = None,\n    prompt_cache_retention: str | None = None,\n    conversation: str | dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; ResponseResource | Response | AsyncIterator[ResponseStreamEvent]:\n    \"\"\"Create a response using the OpenResponses API.\n\n    This implements the OpenResponses specification and returns either\n    `openresponses_types.ResponseResource` (for OpenResponses-compliant providers)\n    or `openai.types.responses.Response` (for providers using OpenAI's native API).\n    If `stream=True`, an iterator of streaming event dicts is returned.\n\n    Args:\n        model: Model identifier for the chosen provider (e.g., model='gpt-4.1-mini' for LLMProvider.OPENAI).\n        input_data: The input payload accepted by provider's Responses API.\n            For OpenAI-compatible providers, this is typically a list mixing\n            text, images, and tool instructions, or a dict per OpenAI spec.\n        tools: Optional tools for tool calling (Python callables or OpenAI tool dicts)\n        tool_choice: Controls which tools the model can call\n        max_output_tokens: Maximum number of output tokens to generate\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        stream: Whether to stream response events\n        instructions: A system (or developer) message inserted into the model's context.\n        max_tool_calls: The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.\n        parallel_tool_calls: Whether to allow the model to run tool calls in parallel.\n        reasoning: Configuration options for reasoning models.\n        text: Configuration options for a text response from the model. Can be plain text or structured JSON data.\n        presence_penalty: Penalizes new tokens based on whether they appear in the text so far.\n        frequency_penalty: Penalizes new tokens based on their frequency in the text so far.\n        truncation: Controls how the service truncates input when it exceeds the model context window.\n        store: Whether to store the response so it can be retrieved later.\n        service_tier: The service tier to use for this request.\n        user: A unique identifier representing your end user.\n        metadata: Key-value pairs for custom metadata (up to 16 pairs).\n        previous_response_id: The ID of the response to use as the prior turn for this request.\n        include: Items to include in the response (e.g., 'reasoning.encrypted_content').\n        background: Whether to run the request in the background and return immediately.\n        safety_identifier: A stable identifier used for safety monitoring and abuse detection.\n        prompt_cache_key: A key to use when reading from or writing to the prompt cache.\n        prompt_cache_retention: How long to retain a prompt cache entry created by this request.\n        conversation: The conversation to associate this response with (ID string or ConversationParam object).\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        Either a `ResponseResource` object (OpenResponses-compliant providers),\n        a `Response` object (non-compliant providers), or an iterator of\n        `ResponseStreamEvent` (streaming).\n\n    Raises:\n        NotImplementedError: If the selected provider does not support the Responses API.\n\n    \"\"\"\n    prepared_tools = None\n    if tools:\n        prepared_tools = prepare_tools(tools, built_in_tools=self.BUILT_IN_TOOLS)\n\n    params = ResponsesParams(\n        model=model,\n        input=input_data,\n        tools=prepared_tools,\n        tool_choice=tool_choice,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        top_p=top_p,\n        stream=stream,\n        instructions=instructions,\n        max_tool_calls=max_tool_calls,\n        parallel_tool_calls=bool(parallel_tool_calls),\n        reasoning=reasoning,\n        text=text,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        truncation=truncation,\n        store=store,\n        service_tier=service_tier,\n        user=user,\n        metadata=metadata,\n        previous_response_id=previous_response_id,\n        include=include,\n        background=background,\n        safety_identifier=safety_identifier,\n        prompt_cache_key=prompt_cache_key,\n        prompt_cache_retention=prompt_cache_retention,\n        conversation=conversation,\n        **kwargs,\n    )\n\n    return await self._aresponses(params)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.aretrieve_batch","title":"<code>aretrieve_batch(batch_id, **kwargs)</code>  <code>async</code>","text":"<p>Retrieve a batch job asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>batch_id</code> <code>str</code> <p>The ID of the batch to retrieve</p> required <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The batch object</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@handle_exceptions()\n@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def aretrieve_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n    \"\"\"Retrieve a batch job asynchronously.\n\n    Args:\n        batch_id: The ID of the batch to retrieve\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The batch object\n\n    \"\"\"\n    return await self._aretrieve_batch(batch_id, **kwargs)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.cancel_batch","title":"<code>cancel_batch(batch_id, **kwargs)</code>","text":"<p>Cancel a batch synchronously.</p> <p>See AnyLLM.acancel_batch</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef cancel_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n    \"\"\"Cancel a batch synchronously.\n\n    See [AnyLLM.acancel_batch][any_llm.any_llm.AnyLLM.acancel_batch]\n    \"\"\"\n    allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n    return run_async_in_sync(self.acancel_batch(batch_id, **kwargs), allow_running_loop=allow_running_loop)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.completion","title":"<code>completion(**kwargs)</code>","text":"<p>Create a chat completion synchronously.</p> <p>See AnyLLM.acompletion</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>def completion(\n    self,\n    **kwargs: Any,\n) -&gt; ChatCompletion | Iterator[ChatCompletionChunk]:\n    \"\"\"Create a chat completion synchronously.\n\n    See [AnyLLM.acompletion][any_llm.any_llm.AnyLLM.acompletion]\n    \"\"\"\n    allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n    response = run_async_in_sync(self.acompletion(**kwargs), allow_running_loop=allow_running_loop)\n    if isinstance(response, ChatCompletion):\n        return response\n\n    return async_iter_to_sync_iter(response)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.create","title":"<code>create(provider, api_key=None, api_base=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a provider instance using the given provider name and config.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>The provider name (e.g., 'openai', 'anthropic')</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>AnyLLM</code> <p>Provider instance for the specified provider</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef create(\n    cls, provider: str | LLMProvider, api_key: str | None = None, api_base: str | None = None, **kwargs: Any\n) -&gt; AnyLLM:\n    \"\"\"Create a provider instance using the given provider name and config.\n\n    Args:\n        provider: The provider name (e.g., 'openai', 'anthropic')\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        Provider instance for the specified provider\n\n    \"\"\"\n    return cls._create_provider(provider, api_key=api_key, api_base=api_base, **kwargs)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.create_batch","title":"<code>create_batch(**kwargs)</code>","text":"<p>Create a batch synchronously.</p> <p>See AnyLLM.acreate_batch</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef create_batch(self, **kwargs: Any) -&gt; Batch:\n    \"\"\"Create a batch synchronously.\n\n    See [AnyLLM.acreate_batch][any_llm.any_llm.AnyLLM.acreate_batch]\n    \"\"\"\n    allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n    return run_async_in_sync(self.acreate_batch(**kwargs), allow_running_loop=allow_running_loop)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.get_all_provider_metadata","title":"<code>get_all_provider_metadata()</code>  <code>classmethod</code>","text":"<p>Get metadata for all supported providers.</p> <p>Returns:</p> Type Description <code>list[ProviderMetadata]</code> <p>List of dictionaries containing provider metadata</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef get_all_provider_metadata(cls) -&gt; list[ProviderMetadata]:\n    \"\"\"Get metadata for all supported providers.\n\n    Returns:\n        List of dictionaries containing provider metadata\n\n    \"\"\"\n    providers: list[ProviderMetadata] = []\n    for provider_key in cls.get_supported_providers():\n        provider_class = cls.get_provider_class(provider_key)\n        metadata = provider_class.get_provider_metadata()\n        providers.append(metadata)\n\n    # Sort providers by name\n    providers.sort(key=lambda x: x.name)\n    return providers\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.get_provider_class","title":"<code>get_provider_class(provider_key)</code>  <code>classmethod</code>","text":"<p>Get the provider class without instantiating it.</p> <p>Parameters:</p> Name Type Description Default <code>provider_key</code> <code>str | LLMProvider</code> <p>The provider key (e.g., 'anthropic', 'openai')</p> required <p>Returns:</p> Type Description <code>type[AnyLLM]</code> <p>The provider class</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef get_provider_class(cls, provider_key: str | LLMProvider) -&gt; type[AnyLLM]:\n    \"\"\"Get the provider class without instantiating it.\n\n    Args:\n        provider_key: The provider key (e.g., 'anthropic', 'openai')\n\n    Returns:\n        The provider class\n\n    \"\"\"\n    provider_key = LLMProvider.from_string(provider_key).value\n\n    provider_class_name = f\"{provider_key.capitalize()}Provider\"\n    provider_module_name = f\"{provider_key}\"\n\n    module_path = f\"any_llm.providers.{provider_module_name}\"\n\n    try:\n        module = importlib.import_module(module_path)\n    except ImportError as e:\n        msg = f\"Could not import module {module_path}: {e!s}. Please ensure the provider is supported by doing AnyLLM.get_supported_providers()\"\n        raise ImportError(msg) from e\n\n    provider_class: type[AnyLLM] = getattr(module, provider_class_name)\n    return provider_class\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.get_provider_enum","title":"<code>get_provider_enum(provider_key)</code>  <code>classmethod</code>","text":"<p>Convert a string provider key to a ProviderName enum.</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef get_provider_enum(cls, provider_key: str) -&gt; LLMProvider:\n    \"\"\"Convert a string provider key to a ProviderName enum.\"\"\"\n    try:\n        return LLMProvider(provider_key)\n    except ValueError as e:\n        supported = [provider.value for provider in LLMProvider]\n        raise UnsupportedProviderError(provider_key, supported) from e\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.get_provider_metadata","title":"<code>get_provider_metadata()</code>  <code>classmethod</code>","text":"<p>Get provider metadata without requiring instantiation.</p> <p>Returns:</p> Type Description <code>ProviderMetadata</code> <p>Dictionary containing provider metadata including name, environment variable,</p> <code>ProviderMetadata</code> <p>documentation URL, and class name.</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef get_provider_metadata(cls) -&gt; ProviderMetadata:\n    \"\"\"Get provider metadata without requiring instantiation.\n\n    Returns:\n        Dictionary containing provider metadata including name, environment variable,\n        documentation URL, and class name.\n\n    \"\"\"\n    return ProviderMetadata(\n        name=cls.PROVIDER_NAME,\n        env_key=cls.ENV_API_KEY_NAME,\n        env_api_base=cls.ENV_API_BASE_NAME,\n        doc_url=cls.PROVIDER_DOCUMENTATION_URL,\n        streaming=cls.SUPPORTS_COMPLETION_STREAMING,\n        reasoning=cls.SUPPORTS_COMPLETION_REASONING,\n        completion=cls.SUPPORTS_COMPLETION,\n        image=cls.SUPPORTS_COMPLETION_IMAGE,\n        pdf=cls.SUPPORTS_COMPLETION_PDF,\n        embedding=cls.SUPPORTS_EMBEDDING,\n        responses=cls.SUPPORTS_RESPONSES,\n        list_models=cls.SUPPORTS_LIST_MODELS,\n        batch_completion=cls.SUPPORTS_BATCH,\n        class_name=cls.__name__,\n    )\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.get_supported_providers","title":"<code>get_supported_providers()</code>  <code>classmethod</code>","text":"<p>Get a list of supported provider keys.</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef get_supported_providers(cls) -&gt; list[str]:\n    \"\"\"Get a list of supported provider keys.\"\"\"\n    return [provider.value for provider in LLMProvider]\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.list_batches","title":"<code>list_batches(after=None, limit=None, **kwargs)</code>","text":"<p>List batches synchronously.</p> <p>See AnyLLM.alist_batches</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef list_batches(\n    self,\n    after: str | None = None,\n    limit: int | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Batch]:\n    \"\"\"List batches synchronously.\n\n    See [AnyLLM.alist_batches][any_llm.any_llm.AnyLLM.alist_batches]\n    \"\"\"\n    allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n    return run_async_in_sync(\n        self.alist_batches(after=after, limit=limit, **kwargs), allow_running_loop=allow_running_loop\n    )\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.responses","title":"<code>responses(**kwargs)</code>","text":"<p>Create a response synchronously.</p> <p>See AnyLLM.aresponses</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>def responses(self, **kwargs: Any) -&gt; ResponseResource | Response | Iterator[ResponseStreamEvent]:\n    \"\"\"Create a response synchronously.\n\n    See [AnyLLM.aresponses][any_llm.any_llm.AnyLLM.aresponses]\n    \"\"\"\n    allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n    response = run_async_in_sync(self.aresponses(**kwargs), allow_running_loop=allow_running_loop)\n    if isinstance(response, (ResponseResource, Response)):\n        return response\n    return async_iter_to_sync_iter(response)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.retrieve_batch","title":"<code>retrieve_batch(batch_id, **kwargs)</code>","text":"<p>Retrieve a batch synchronously.</p> <p>See AnyLLM.aretrieve_batch</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef retrieve_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n    \"\"\"Retrieve a batch synchronously.\n\n    See [AnyLLM.aretrieve_batch][any_llm.any_llm.AnyLLM.aretrieve_batch]\n    \"\"\"\n    allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n    return run_async_in_sync(self.aretrieve_batch(batch_id, **kwargs), allow_running_loop=allow_running_loop)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.split_model_provider","title":"<code>split_model_provider(model)</code>  <code>classmethod</code>","text":"<p>Extract the provider key from the model identifier.</p> <p>Supports both new format 'provider:model' (e.g., 'mistral:mistral-small') and legacy format 'provider/model' (e.g., 'mistral/mistral-small').</p> <p>The legacy format will be deprecated in version 1.0.</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef split_model_provider(cls, model: str) -&gt; tuple[LLMProvider, str]:\n    \"\"\"Extract the provider key from the model identifier.\n\n    Supports both new format 'provider:model' (e.g., 'mistral:mistral-small')\n    and legacy format 'provider/model' (e.g., 'mistral/mistral-small').\n\n    The legacy format will be deprecated in version 1.0.\n    \"\"\"\n    colon_index = model.find(\":\")\n    slash_index = model.find(\"/\")\n\n    # Determine which delimiter comes first\n    if colon_index != -1 and (slash_index == -1 or colon_index &lt; slash_index):\n        # The colon came first, so it's using the new syntax.\n        provider, model_name = model.split(\":\", 1)\n    elif slash_index != -1:\n        # Slash comes first, so it's the legacy syntax\n        warnings.warn(\n            f\"Model format 'provider/model' is deprecated and will be removed in version 1.0. \"\n            f\"Please use 'provider:model' format instead. Got: '{model}'\",\n            DeprecationWarning,\n            stacklevel=3,\n        )\n        provider, model_name = model.split(\"/\", 1)\n    else:\n        msg = f\"Invalid model format. Expected 'provider:model' or 'provider/model', got '{model}'\"\n        raise ValueError(msg)\n\n    if not provider or not model_name:\n        msg = f\"Invalid model format. Expected 'provider:model' or 'provider/model', got '{model}'\"\n        raise ValueError(msg)\n    return cls.get_provider_enum(provider), model_name\n</code></pre>"},{"location":"api/batch/","title":"Batch","text":"<p>Experimental API</p> <p>The Batch API is experimental and subject to breaking changes in future versions. Use with caution in production environments.</p> <p>The Batch API allows you to process multiple requests asynchronously at a lower cost.</p>"},{"location":"api/batch/#file-path-interface","title":"File Path Interface","text":"<p>The <code>any-llm</code> batch API requires you to pass a path to a local JSONL file containing your batch requests. The provider implementation automatically handles uploading and file management as needed.</p> <p>Different providers handle batch processing differently:</p> <ul> <li>OpenAI: Requires uploading a file first, then creating a batch with the file ID</li> <li>Anthropic (future): Expects file content passed directly in the request</li> <li>Other providers: May have their own unique requirements</li> </ul> <p>By accepting a local file path, <code>any-llm</code> abstracts these provider differences and handles the implementation details automatically.</p>"},{"location":"api/batch/#any_llm.api.create_batch","title":"<code>any_llm.api.create_batch(provider, input_file_path, endpoint, *, completion_window='24h', metadata=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>Create a batch job.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>input_file_path</code> <code>str</code> <p>Path to a local file containing batch requests in JSONL format.</p> required <code>endpoint</code> <code>str</code> <p>The endpoint to be used for all requests (e.g., '/v1/chat/completions')</p> required <code>completion_window</code> <code>str</code> <p>The time frame within which the batch should be processed (default: '24h')</p> <code>'24h'</code> <code>metadata</code> <code>dict[str, str] | None</code> <p>Optional custom metadata for the batch</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The created batch object</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef create_batch(\n    provider: str | LLMProvider,\n    input_file_path: str,\n    endpoint: str,\n    *,\n    completion_window: str = \"24h\",\n    metadata: dict[str, str] | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Create a batch job.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        input_file_path: Path to a local file containing batch requests in JSONL format.\n        endpoint: The endpoint to be used for all requests (e.g., '/v1/chat/completions')\n        completion_window: The time frame within which the batch should be processed (default: '24h')\n        metadata: Optional custom metadata for the batch\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The created batch object\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return llm.create_batch(\n        input_file_path=input_file_path,\n        endpoint=endpoint,\n        completion_window=completion_window,\n        metadata=metadata,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/batch/#any_llm.api.acreate_batch","title":"<code>any_llm.api.acreate_batch(provider, input_file_path, endpoint, *, completion_window='24h', metadata=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Create a batch job asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>input_file_path</code> <code>str</code> <p>Path to a local file containing batch requests in JSONL format.</p> required <code>endpoint</code> <code>str</code> <p>The endpoint to be used for all requests (e.g., '/v1/chat/completions')</p> required <code>completion_window</code> <code>str</code> <p>The time frame within which the batch should be processed (default: '24h')</p> <code>'24h'</code> <code>metadata</code> <code>dict[str, str] | None</code> <p>Optional custom metadata for the batch</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The created batch object</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def acreate_batch(\n    provider: str | LLMProvider,\n    input_file_path: str,\n    endpoint: str,\n    *,\n    completion_window: str = \"24h\",\n    metadata: dict[str, str] | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Create a batch job asynchronously.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        input_file_path: Path to a local file containing batch requests in JSONL format.\n        endpoint: The endpoint to be used for all requests (e.g., '/v1/chat/completions')\n        completion_window: The time frame within which the batch should be processed (default: '24h')\n        metadata: Optional custom metadata for the batch\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The created batch object\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return await llm.acreate_batch(\n        input_file_path=input_file_path,\n        endpoint=endpoint,\n        completion_window=completion_window,\n        metadata=metadata,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/batch/#any_llm.api.retrieve_batch","title":"<code>any_llm.api.retrieve_batch(provider, batch_id, *, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>Retrieve a batch job.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>batch_id</code> <code>str</code> <p>The ID of the batch to retrieve</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The batch object</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef retrieve_batch(\n    provider: str | LLMProvider,\n    batch_id: str,\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Retrieve a batch job.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        batch_id: The ID of the batch to retrieve\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The batch object\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return llm.retrieve_batch(batch_id, **kwargs)\n</code></pre>"},{"location":"api/batch/#any_llm.api.aretrieve_batch","title":"<code>any_llm.api.aretrieve_batch(provider, batch_id, *, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Retrieve a batch job asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>batch_id</code> <code>str</code> <p>The ID of the batch to retrieve</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The batch object</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def aretrieve_batch(\n    provider: str | LLMProvider,\n    batch_id: str,\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Retrieve a batch job asynchronously.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        batch_id: The ID of the batch to retrieve\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The batch object\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return await llm.aretrieve_batch(batch_id, **kwargs)\n</code></pre>"},{"location":"api/batch/#any_llm.api.cancel_batch","title":"<code>any_llm.api.cancel_batch(provider, batch_id, *, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>Cancel a batch job.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>batch_id</code> <code>str</code> <p>The ID of the batch to cancel</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The cancelled batch object</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef cancel_batch(\n    provider: str | LLMProvider,\n    batch_id: str,\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Cancel a batch job.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        batch_id: The ID of the batch to cancel\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The cancelled batch object\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return llm.cancel_batch(batch_id, **kwargs)\n</code></pre>"},{"location":"api/batch/#any_llm.api.acancel_batch","title":"<code>any_llm.api.acancel_batch(provider, batch_id, *, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Cancel a batch job asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>batch_id</code> <code>str</code> <p>The ID of the batch to cancel</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The cancelled batch object</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def acancel_batch(\n    provider: str | LLMProvider,\n    batch_id: str,\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Cancel a batch job asynchronously.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        batch_id: The ID of the batch to cancel\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The cancelled batch object\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return await llm.acancel_batch(batch_id, **kwargs)\n</code></pre>"},{"location":"api/batch/#any_llm.api.list_batches","title":"<code>any_llm.api.list_batches(provider, *, after=None, limit=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>List batch jobs.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>after</code> <code>str | None</code> <p>A cursor for pagination. Returns batches after this batch ID.</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>Maximum number of batches to return (default: 20)</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Sequence[Batch]</code> <p>A list of batch objects</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef list_batches(\n    provider: str | LLMProvider,\n    *,\n    after: str | None = None,\n    limit: int | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Batch]:\n    \"\"\"List batch jobs.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        after: A cursor for pagination. Returns batches after this batch ID.\n        limit: Maximum number of batches to return (default: 20)\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        A list of batch objects\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return llm.list_batches(after=after, limit=limit, **kwargs)\n</code></pre>"},{"location":"api/batch/#any_llm.api.alist_batches","title":"<code>any_llm.api.alist_batches(provider, *, after=None, limit=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>List batch jobs asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>after</code> <code>str | None</code> <p>A cursor for pagination. Returns batches after this batch ID.</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>Maximum number of batches to return (default: 20)</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Sequence[Batch]</code> <p>A list of batch objects</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def alist_batches(\n    provider: str | LLMProvider,\n    *,\n    after: str | None = None,\n    limit: int | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Batch]:\n    \"\"\"List batch jobs asynchronously.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        after: A cursor for pagination. Returns batches after this batch ID.\n        limit: Maximum number of batches to return (default: 20)\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        A list of batch objects\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return await llm.alist_batches(after=after, limit=limit, **kwargs)\n</code></pre>"},{"location":"api/completion/","title":"Completion","text":""},{"location":"api/completion/#completion","title":"Completion","text":""},{"location":"api/completion/#any_llm.api.completion","title":"<code>any_llm.api.completion(model, messages, *, provider=None, tools=None, tool_choice=None, temperature=None, top_p=None, max_tokens=None, response_format=None, stream=None, n=None, stop=None, presence_penalty=None, frequency_penalty=None, seed=None, api_key=None, api_base=None, user=None, parallel_tool_calls=None, logprobs=None, top_logprobs=None, logit_bias=None, stream_options=None, max_completion_tokens=None, reasoning_effort='auto', client_args=None, **kwargs)</code>","text":"<p>Create a chat completion.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier. Recommended: Use with separate <code>provider</code> parameter (e.g., model='gpt-4', provider='openai'). Alternative: Combined format 'provider:model' (e.g., 'openai:gpt-4'). Legacy format 'provider/model' is also supported but deprecated.</p> required <code>provider</code> <code>str | LLMProvider | None</code> <p>Recommended: Provider name to use for the request (e.g., 'openai', 'mistral'). When provided, the model parameter should contain only the model name.</p> <code>None</code> <code>messages</code> <code>list[dict[str, Any] | ChatCompletionMessage]</code> <p>List of messages for the conversation</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>List of tools for tool calling. Can be Python callables or OpenAI tool format dicts</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>max_tokens</code> <code>int | None</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>response_format</code> <code>dict[str, Any] | type[BaseModel] | None</code> <p>Format specification for the response</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream the response</p> <code>None</code> <code>n</code> <code>int | None</code> <p>Number of completions to generate</p> <code>None</code> <code>stop</code> <code>str | list[str] | None</code> <p>Stop sequences for generation</p> <code>None</code> <code>presence_penalty</code> <code>float | None</code> <p>Penalize new tokens based on presence in text</p> <code>None</code> <code>frequency_penalty</code> <code>float | None</code> <p>Penalize new tokens based on frequency in text</p> <code>None</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducible results</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>user</code> <code>str | None</code> <p>Unique identifier for the end user</p> <code>None</code> <code>parallel_tool_calls</code> <code>bool | None</code> <p>Whether to allow parallel tool calls</p> <code>None</code> <code>logprobs</code> <code>bool | None</code> <p>Include token-level log probabilities in the response</p> <code>None</code> <code>top_logprobs</code> <code>int | None</code> <p>Number of alternatives to return when logprobs are requested</p> <code>None</code> <code>logit_bias</code> <code>dict[str, float] | None</code> <p>Bias the likelihood of specified tokens during generation</p> <code>None</code> <code>stream_options</code> <code>dict[str, Any] | None</code> <p>Additional options controlling streaming behavior</p> <code>None</code> <code>max_completion_tokens</code> <code>int | None</code> <p>Maximum number of tokens for the completion</p> <code>None</code> <code>reasoning_effort</code> <code>ReasoningEffort | None</code> <p>Reasoning effort level for models that support it. \"auto\" will map to each provider's default.</p> <code>'auto'</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatCompletion | Iterator[ChatCompletionChunk]</code> <p>The completion response from the provider</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def completion(\n    model: str,\n    messages: list[dict[str, Any] | ChatCompletionMessage],\n    *,\n    provider: str | LLMProvider | None = None,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    max_tokens: int | None = None,\n    response_format: dict[str, Any] | type[BaseModel] | None = None,\n    stream: bool | None = None,\n    n: int | None = None,\n    stop: str | list[str] | None = None,\n    presence_penalty: float | None = None,\n    frequency_penalty: float | None = None,\n    seed: int | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    user: str | None = None,\n    parallel_tool_calls: bool | None = None,\n    logprobs: bool | None = None,\n    top_logprobs: int | None = None,\n    logit_bias: dict[str, float] | None = None,\n    stream_options: dict[str, Any] | None = None,\n    max_completion_tokens: int | None = None,\n    reasoning_effort: ReasoningEffort | None = \"auto\",\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; ChatCompletion | Iterator[ChatCompletionChunk]:\n    \"\"\"Create a chat completion.\n\n    Args:\n        model: Model identifier. **Recommended**: Use with separate `provider` parameter (e.g., model='gpt-4', provider='openai').\n            **Alternative**: Combined format 'provider:model' (e.g., 'openai:gpt-4').\n            Legacy format 'provider/model' is also supported but deprecated.\n        provider: **Recommended**: Provider name to use for the request (e.g., 'openai', 'mistral').\n            When provided, the model parameter should contain only the model name.\n        messages: List of messages for the conversation\n        tools: List of tools for tool calling. Can be Python callables or OpenAI tool format dicts\n        tool_choice: Controls which tools the model can call\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        max_tokens: Maximum number of tokens to generate\n        response_format: Format specification for the response\n        stream: Whether to stream the response\n        n: Number of completions to generate\n        stop: Stop sequences for generation\n        presence_penalty: Penalize new tokens based on presence in text\n        frequency_penalty: Penalize new tokens based on frequency in text\n        seed: Random seed for reproducible results\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        user: Unique identifier for the end user\n        parallel_tool_calls: Whether to allow parallel tool calls\n        logprobs: Include token-level log probabilities in the response\n        top_logprobs: Number of alternatives to return when logprobs are requested\n        logit_bias: Bias the likelihood of specified tokens during generation\n        stream_options: Additional options controlling streaming behavior\n        max_completion_tokens: Maximum number of tokens for the completion\n        reasoning_effort: Reasoning effort level for models that support it. \"auto\" will map to each provider's default.\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        The completion response from the provider\n\n    \"\"\"\n    if provider is None:\n        provider_key, model_id = AnyLLM.split_model_provider(model)\n    else:\n        provider_key = LLMProvider.from_string(provider)\n        model_id = model\n\n    llm = AnyLLM.create(\n        provider_key,\n        api_key=api_key,\n        api_base=api_base,\n        **client_args or {},\n    )\n    return llm.completion(\n        model=model_id,\n        messages=messages,\n        tools=tools,\n        tool_choice=tool_choice,\n        temperature=temperature,\n        top_p=top_p,\n        max_tokens=max_tokens,\n        response_format=response_format,\n        stream=stream,\n        n=n,\n        stop=stop,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        seed=seed,\n        user=user,\n        parallel_tool_calls=parallel_tool_calls,\n        logprobs=logprobs,\n        top_logprobs=top_logprobs,\n        logit_bias=logit_bias,\n        stream_options=stream_options,\n        max_completion_tokens=max_completion_tokens,\n        reasoning_effort=reasoning_effort,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/completion/#any_llm.api.acompletion","title":"<code>any_llm.api.acompletion(model, messages, *, provider=None, tools=None, tool_choice=None, temperature=None, top_p=None, max_tokens=None, response_format=None, stream=None, n=None, stop=None, presence_penalty=None, frequency_penalty=None, seed=None, api_key=None, api_base=None, user=None, parallel_tool_calls=None, logprobs=None, top_logprobs=None, logit_bias=None, stream_options=None, max_completion_tokens=None, reasoning_effort='auto', client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Create a chat completion asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier. Recommended: Use with separate <code>provider</code> parameter (e.g., model='gpt-4', provider='openai'). Alternative: Combined format 'provider:model' (e.g., 'openai:gpt-4'). Legacy format 'provider/model' is also supported but deprecated.</p> required <code>provider</code> <code>str | LLMProvider | None</code> <p>Recommended: Provider name to use for the request (e.g., 'openai', 'mistral'). When provided, the model parameter should contain only the model name.</p> <code>None</code> <code>messages</code> <code>list[dict[str, Any] | ChatCompletionMessage]</code> <p>List of messages for the conversation</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>List of tools for tool calling. Can be Python callables or OpenAI tool format dicts</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>max_tokens</code> <code>int | None</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>response_format</code> <code>dict[str, Any] | type[BaseModel] | None</code> <p>Format specification for the response</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream the response</p> <code>None</code> <code>n</code> <code>int | None</code> <p>Number of completions to generate</p> <code>None</code> <code>stop</code> <code>str | list[str] | None</code> <p>Stop sequences for generation</p> <code>None</code> <code>presence_penalty</code> <code>float | None</code> <p>Penalize new tokens based on presence in text</p> <code>None</code> <code>frequency_penalty</code> <code>float | None</code> <p>Penalize new tokens based on frequency in text</p> <code>None</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducible results</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>user</code> <code>str | None</code> <p>Unique identifier for the end user</p> <code>None</code> <code>parallel_tool_calls</code> <code>bool | None</code> <p>Whether to allow parallel tool calls</p> <code>None</code> <code>logprobs</code> <code>bool | None</code> <p>Include token-level log probabilities in the response</p> <code>None</code> <code>top_logprobs</code> <code>int | None</code> <p>Number of alternatives to return when logprobs are requested</p> <code>None</code> <code>logit_bias</code> <code>dict[str, float] | None</code> <p>Bias the likelihood of specified tokens during generation</p> <code>None</code> <code>stream_options</code> <code>dict[str, Any] | None</code> <p>Additional options controlling streaming behavior</p> <code>None</code> <code>max_completion_tokens</code> <code>int | None</code> <p>Maximum number of tokens for the completion</p> <code>None</code> <code>reasoning_effort</code> <code>ReasoningEffort | None</code> <p>Reasoning effort level for models that support it. \"auto\" will map to each provider's default.</p> <code>'auto'</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatCompletion | AsyncIterator[ChatCompletionChunk]</code> <p>The completion response from the provider</p> Source code in <code>src/any_llm/api.py</code> <pre><code>async def acompletion(\n    model: str,\n    messages: list[dict[str, Any] | ChatCompletionMessage],\n    *,\n    provider: str | LLMProvider | None = None,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    max_tokens: int | None = None,\n    response_format: dict[str, Any] | type[BaseModel] | None = None,\n    stream: bool | None = None,\n    n: int | None = None,\n    stop: str | list[str] | None = None,\n    presence_penalty: float | None = None,\n    frequency_penalty: float | None = None,\n    seed: int | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    user: str | None = None,\n    parallel_tool_calls: bool | None = None,\n    logprobs: bool | None = None,\n    top_logprobs: int | None = None,\n    logit_bias: dict[str, float] | None = None,\n    stream_options: dict[str, Any] | None = None,\n    max_completion_tokens: int | None = None,\n    reasoning_effort: ReasoningEffort | None = \"auto\",\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; ChatCompletion | AsyncIterator[ChatCompletionChunk]:\n    \"\"\"Create a chat completion asynchronously.\n\n    Args:\n        model: Model identifier. **Recommended**: Use with separate `provider` parameter (e.g., model='gpt-4', provider='openai').\n            **Alternative**: Combined format 'provider:model' (e.g., 'openai:gpt-4').\n            Legacy format 'provider/model' is also supported but deprecated.\n        provider: **Recommended**: Provider name to use for the request (e.g., 'openai', 'mistral').\n            When provided, the model parameter should contain only the model name.\n        messages: List of messages for the conversation\n        tools: List of tools for tool calling. Can be Python callables or OpenAI tool format dicts\n        tool_choice: Controls which tools the model can call\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        max_tokens: Maximum number of tokens to generate\n        response_format: Format specification for the response\n        stream: Whether to stream the response\n        n: Number of completions to generate\n        stop: Stop sequences for generation\n        presence_penalty: Penalize new tokens based on presence in text\n        frequency_penalty: Penalize new tokens based on frequency in text\n        seed: Random seed for reproducible results\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        user: Unique identifier for the end user\n        parallel_tool_calls: Whether to allow parallel tool calls\n        logprobs: Include token-level log probabilities in the response\n        top_logprobs: Number of alternatives to return when logprobs are requested\n        logit_bias: Bias the likelihood of specified tokens during generation\n        stream_options: Additional options controlling streaming behavior\n        max_completion_tokens: Maximum number of tokens for the completion\n        reasoning_effort: Reasoning effort level for models that support it. \"auto\" will map to each provider's default.\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        The completion response from the provider\n\n    \"\"\"\n    if provider is None:\n        provider_key, model_id = AnyLLM.split_model_provider(model)\n    else:\n        provider_key = LLMProvider.from_string(provider)\n        model_id = model\n\n    llm = AnyLLM.create(\n        provider_key,\n        api_key=api_key,\n        api_base=api_base,\n        **client_args or {},\n    )\n    return await llm.acompletion(\n        model=model_id,\n        messages=messages,\n        tools=tools,\n        tool_choice=tool_choice,\n        temperature=temperature,\n        top_p=top_p,\n        max_tokens=max_tokens,\n        response_format=response_format,\n        stream=stream,\n        n=n,\n        stop=stop,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        seed=seed,\n        user=user,\n        parallel_tool_calls=parallel_tool_calls,\n        logprobs=logprobs,\n        top_logprobs=top_logprobs,\n        logit_bias=logit_bias,\n        stream_options=stream_options,\n        max_completion_tokens=max_completion_tokens,\n        reasoning_effort=reasoning_effort,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/embedding/","title":"Embedding","text":""},{"location":"api/embedding/#embedding","title":"Embedding","text":""},{"location":"api/embedding/#any_llm.api.embedding","title":"<code>any_llm.api.embedding(model, inputs, *, provider=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>Create an embedding.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier. Recommended: Use with separate <code>provider</code> parameter (e.g., model='gpt-4', provider='openai'). Alternative: Combined format 'provider:model' (e.g., 'openai:gpt-4'). Legacy format 'provider/model' is also supported but deprecated.</p> required <code>provider</code> <code>str | LLMProvider | None</code> <p>Recommended: Provider name to use for the request (e.g., 'openai', 'mistral'). When provided, the model parameter should contain only the model name.</p> <code>None</code> <code>inputs</code> <code>str | list[str]</code> <p>The input text to embed</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CreateEmbeddingResponse</code> <p>The embedding of the input text</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def embedding(\n    model: str,\n    inputs: str | list[str],\n    *,\n    provider: str | LLMProvider | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; CreateEmbeddingResponse:\n    \"\"\"Create an embedding.\n\n    Args:\n        model: Model identifier. **Recommended**: Use with separate `provider` parameter (e.g., model='gpt-4', provider='openai').\n            **Alternative**: Combined format 'provider:model' (e.g., 'openai:gpt-4').\n            Legacy format 'provider/model' is also supported but deprecated.\n        provider: **Recommended**: Provider name to use for the request (e.g., 'openai', 'mistral').\n            When provided, the model parameter should contain only the model name.\n        inputs: The input text to embed\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        The embedding of the input text\n\n    \"\"\"\n    if provider is None:\n        provider_key, model_name = AnyLLM.split_model_provider(model)\n    else:\n        provider_key = LLMProvider.from_string(provider)\n        model_name = model\n\n    llm = AnyLLM.create(provider_key, api_key=api_key, api_base=api_base, **client_args or {})\n    return llm._embedding(model_name, inputs, **kwargs)\n</code></pre>"},{"location":"api/embedding/#any_llm.api.aembedding","title":"<code>any_llm.api.aembedding(model, inputs, *, provider=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Create an embedding asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier in format 'provider/model' (e.g., 'openai/text-embedding-3-small'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.</p> required <code>provider</code> <code>str | LLMProvider | None</code> <p>Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.</p> <code>None</code> <code>inputs</code> <code>str | list[str]</code> <p>The input text to embed</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CreateEmbeddingResponse</code> <p>The embedding of the input text</p> Source code in <code>src/any_llm/api.py</code> <pre><code>async def aembedding(\n    model: str,\n    inputs: str | list[str],\n    *,\n    provider: str | LLMProvider | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; CreateEmbeddingResponse:\n    \"\"\"Create an embedding asynchronously.\n\n    Args:\n        model: Model identifier in format 'provider/model' (e.g., 'openai/text-embedding-3-small'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.\n        provider: Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.\n        inputs: The input text to embed\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        The embedding of the input text\n\n    \"\"\"\n    if provider is None:\n        provider_key, model_name = AnyLLM.split_model_provider(model)\n    else:\n        provider_key = LLMProvider.from_string(provider)\n        model_name = model\n\n    llm = AnyLLM.create(provider_key, api_key=api_key, api_base=api_base, **client_args or {})\n    return await llm._aembedding(model_name, inputs, **kwargs)\n</code></pre>"},{"location":"api/exceptions/","title":"Exception Handling","text":""},{"location":"api/exceptions/#any_llm.exceptions.AnyLLMError","title":"<code>AnyLLMError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all any-llm errors.</p> <p>All custom exceptions in any-llm inherit from this class. It preserves the original exception for debugging while providing a unified interface.</p> <p>Attributes:</p> Name Type Description <code>message</code> <p>Human-readable error message</p> <code>original_exception</code> <p>The original SDK exception that triggered this error</p> <code>provider_name</code> <p>Name of the provider that raised the error (if available)</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class AnyLLMError(Exception):\n    \"\"\"Base exception for all any-llm errors.\n\n    All custom exceptions in any-llm inherit from this class. It preserves\n    the original exception for debugging while providing a unified interface.\n\n    Attributes:\n        message: Human-readable error message\n        original_exception: The original SDK exception that triggered this error\n        provider_name: Name of the provider that raised the error (if available)\n\n    \"\"\"\n\n    default_message: str = \"An error occurred\"\n\n    def __init__(\n        self,\n        message: str | None = None,\n        original_exception: Exception | None = None,\n        provider_name: str | None = None,\n    ) -&gt; None:\n        self.message = message or self.default_message\n        super().__init__(self.message)\n        self.original_exception = original_exception\n        self.provider_name = provider_name\n\n    @override\n    def __str__(self) -&gt; str:\n        \"\"\"Return string representation of the exception.\"\"\"\n        if self.provider_name:\n            return f\"[{self.provider_name}] {self.message}\"\n        return self.message\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.AnyLLMError.__str__","title":"<code>__str__()</code>","text":"<p>Return string representation of the exception.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>@override\ndef __str__(self) -&gt; str:\n    \"\"\"Return string representation of the exception.\"\"\"\n    if self.provider_name:\n        return f\"[{self.provider_name}] {self.message}\"\n    return self.message\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.AuthenticationError","title":"<code>AuthenticationError</code>","text":"<p>               Bases: <code>AnyLLMError</code></p> <p>Raised when authentication with the provider fails.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class AuthenticationError(AnyLLMError):\n    \"\"\"Raised when authentication with the provider fails.\"\"\"\n\n    default_message = \"Authentication failed\"\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.ContentFilterError","title":"<code>ContentFilterError</code>","text":"<p>               Bases: <code>AnyLLMError</code></p> <p>Raised when content is blocked by the provider's safety filter.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class ContentFilterError(AnyLLMError):\n    \"\"\"Raised when content is blocked by the provider's safety filter.\"\"\"\n\n    default_message = \"Content blocked by safety filter\"\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.ContextLengthExceededError","title":"<code>ContextLengthExceededError</code>","text":"<p>               Bases: <code>AnyLLMError</code></p> <p>Raised when the input exceeds the model's maximum context length.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class ContextLengthExceededError(AnyLLMError):\n    \"\"\"Raised when the input exceeds the model's maximum context length.\"\"\"\n\n    default_message = \"Context length exceeded\"\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.InvalidRequestError","title":"<code>InvalidRequestError</code>","text":"<p>               Bases: <code>AnyLLMError</code></p> <p>Raised when the request to the provider is invalid.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class InvalidRequestError(AnyLLMError):\n    \"\"\"Raised when the request to the provider is invalid.\"\"\"\n\n    default_message = \"Invalid request\"\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.MissingApiKeyError","title":"<code>MissingApiKeyError</code>","text":"<p>               Bases: <code>AnyLLMError</code></p> <p>Raised when a required API key is not provided.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class MissingApiKeyError(AnyLLMError):\n    \"\"\"Raised when a required API key is not provided.\"\"\"\n\n    def __init__(self, provider_name: str, env_var_name: str) -&gt; None:\n        self.env_var_name = env_var_name\n\n        message = (\n            f\"No {provider_name} API key provided. \"\n            f\"Please provide it in the config or set the {env_var_name} environment variable.\"\n        )\n\n        super().__init__(message, provider_name=provider_name)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.ModelNotFoundError","title":"<code>ModelNotFoundError</code>","text":"<p>               Bases: <code>AnyLLMError</code></p> <p>Raised when the requested model is not found or not available.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class ModelNotFoundError(AnyLLMError):\n    \"\"\"Raised when the requested model is not found or not available.\"\"\"\n\n    default_message = \"Model not found\"\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.ProviderError","title":"<code>ProviderError</code>","text":"<p>               Bases: <code>AnyLLMError</code></p> <p>Raised when the provider encounters an internal error.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class ProviderError(AnyLLMError):\n    \"\"\"Raised when the provider encounters an internal error.\"\"\"\n\n    default_message = \"Provider error\"\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.RateLimitError","title":"<code>RateLimitError</code>","text":"<p>               Bases: <code>AnyLLMError</code></p> <p>Raised when the API rate limit is exceeded.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class RateLimitError(AnyLLMError):\n    \"\"\"Raised when the API rate limit is exceeded.\"\"\"\n\n    default_message = \"Rate limit exceeded\"\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedParameterError","title":"<code>UnsupportedParameterError</code>","text":"<p>               Bases: <code>AnyLLMError</code></p> <p>Raised when a parameter is not supported by the provider.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class UnsupportedParameterError(AnyLLMError):\n    \"\"\"Raised when a parameter is not supported by the provider.\"\"\"\n\n    def __init__(self, parameter_name: str, provider_name: str, additional_message: str | None = None) -&gt; None:\n        self.parameter_name = parameter_name\n\n        message = f\"'{parameter_name}' is not supported for {provider_name}\"\n        if additional_message is not None:\n            message = f\"{message}.\\n{additional_message}\"\n\n        super().__init__(message, provider_name=provider_name)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedProviderError","title":"<code>UnsupportedProviderError</code>","text":"<p>               Bases: <code>AnyLLMError</code></p> <p>Raised when an unsupported provider is specified.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class UnsupportedProviderError(AnyLLMError):\n    \"\"\"Raised when an unsupported provider is specified.\"\"\"\n\n    def __init__(self, provider_key: str, supported_providers: list[str]) -&gt; None:\n        self.provider_key = provider_key\n        self.supported_providers = supported_providers\n\n        message = f\"'{provider_key}' is not a supported provider. Supported providers: {', '.join(supported_providers)}\"\n\n        super().__init__(message)\n</code></pre>"},{"location":"api/list_models/","title":"List Models","text":""},{"location":"api/list_models/#models","title":"Models","text":""},{"location":"api/list_models/#any_llm.api.list_models","title":"<code>any_llm.api.list_models(provider, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>List available models for a provider.</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def list_models(\n    provider: str | LLMProvider,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Model]:\n    \"\"\"List available models for a provider.\"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return llm.list_models(**kwargs)\n</code></pre>"},{"location":"api/list_models/#any_llm.api.alist_models","title":"<code>any_llm.api.alist_models(provider, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>List available models for a provider asynchronously.</p> Source code in <code>src/any_llm/api.py</code> <pre><code>async def alist_models(\n    provider: str | LLMProvider,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Model]:\n    \"\"\"List available models for a provider asynchronously.\"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return await llm.alist_models(**kwargs)\n</code></pre>"},{"location":"api/responses/","title":"Responses","text":""},{"location":"api/responses/#openresponses-api","title":"OpenResponses API","text":"<p>The Responses API in any-llm implements the OpenResponses specification\u2014an open-source standard for building multi-provider, interoperable LLM interfaces for agentic AI systems.</p> <p>Learn More</p> <ul> <li>OpenResponses Specification</li> <li>OpenResponses Reference</li> <li>HuggingFace Responses API Guide</li> </ul>"},{"location":"api/responses/#return-types","title":"Return Types","text":"<p>The <code>responses()</code> and <code>aresponses()</code> functions return different types depending on the provider's level of OpenResponses compliance:</p> Return Type When Returned <code>openresponses_types.ResponseResource</code> Providers fully compliant with the OpenResponses specification <code>openai.types.responses.Response</code> Providers using OpenAI's native Responses API (not yet fully OpenResponses-compliant) <code>Iterator[dict]</code> / <code>AsyncIterator[dict]</code> When <code>stream=True</code> is set <p>Both <code>ResponseResource</code> and <code>Response</code> share a similar structure, so in many cases you can access common fields like <code>output</code> without type checking.</p>"},{"location":"api/responses/#any_llm.api.responses","title":"<code>any_llm.api.responses(model, input_data, *, provider=None, tools=None, tool_choice=None, max_output_tokens=None, temperature=None, top_p=None, stream=None, api_key=None, api_base=None, instructions=None, max_tool_calls=None, parallel_tool_calls=None, reasoning=None, text=None, presence_penalty=None, frequency_penalty=None, truncation=None, store=None, service_tier=None, user=None, metadata=None, previous_response_id=None, include=None, background=None, safety_identifier=None, prompt_cache_key=None, prompt_cache_retention=None, conversation=None, client_args=None, **kwargs)</code>","text":"<p>Create a response using the OpenResponses API.</p> <p>This implements the OpenResponses specification and returns either <code>openresponses_types.ResponseResource</code> (for OpenResponses-compliant providers) or <code>openai.types.responses.Response</code> (for providers using OpenAI's native API). If <code>stream=True</code>, an iterator of <code>any_llm.types.responses.ResponseStreamEvent</code> items is returned.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.</p> required <code>provider</code> <code>str | LLMProvider | None</code> <p>Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.</p> <code>None</code> <code>input_data</code> <code>str | ResponseInputParam</code> <p>The input payload accepted by provider's Responses API. For OpenAI-compatible providers, this is typically a list mixing text, images, and tool instructions, or a dict per OpenAI spec.</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>Optional tools for tool calling (Python callables or OpenAI tool dicts)</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>max_output_tokens</code> <code>int | None</code> <p>Maximum number of output tokens to generate</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream response events</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>instructions</code> <code>str | None</code> <p>A system (or developer) message inserted into the model's context.</p> <code>None</code> <code>max_tool_calls</code> <code>int | None</code> <p>The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.</p> <code>None</code> <code>parallel_tool_calls</code> <code>int | None</code> <p>Whether to allow the model to run tool calls in parallel.</p> <code>None</code> <code>reasoning</code> <code>Any | None</code> <p>Configuration options for reasoning models.</p> <code>None</code> <code>text</code> <code>Any | None</code> <p>Configuration options for a text response from the model. Can be plain text or structured JSON data.</p> <code>None</code> <code>presence_penalty</code> <code>float | None</code> <p>Penalizes new tokens based on whether they appear in the text so far.</p> <code>None</code> <code>frequency_penalty</code> <code>float | None</code> <p>Penalizes new tokens based on their frequency in the text so far.</p> <code>None</code> <code>truncation</code> <code>str | None</code> <p>Controls how the service truncates input when it exceeds the model context window.</p> <code>None</code> <code>store</code> <code>bool | None</code> <p>Whether to store the response so it can be retrieved later.</p> <code>None</code> <code>service_tier</code> <code>str | None</code> <p>The service tier to use for this request.</p> <code>None</code> <code>user</code> <code>str | None</code> <p>A unique identifier representing your end user.</p> <code>None</code> <code>metadata</code> <code>dict[str, str] | None</code> <p>Key-value pairs for custom metadata (up to 16 pairs).</p> <code>None</code> <code>previous_response_id</code> <code>str | None</code> <p>The ID of the response to use as the prior turn for this request.</p> <code>None</code> <code>include</code> <code>list[str] | None</code> <p>Items to include in the response (e.g., 'reasoning.encrypted_content').</p> <code>None</code> <code>background</code> <code>bool | None</code> <p>Whether to run the request in the background and return immediately.</p> <code>None</code> <code>safety_identifier</code> <code>str | None</code> <p>A stable identifier used for safety monitoring and abuse detection.</p> <code>None</code> <code>prompt_cache_key</code> <code>str | None</code> <p>A key to use when reading from or writing to the prompt cache.</p> <code>None</code> <code>prompt_cache_retention</code> <code>str | None</code> <p>How long to retain a prompt cache entry created by this request.</p> <code>None</code> <code>conversation</code> <code>str | dict[str, Any] | None</code> <p>The conversation to associate this response with (ID string or ConversationParam object).</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ResponseResource | Response | Iterator[ResponseStreamEvent]</code> <p>Either a <code>ResponseResource</code> object (OpenResponses-compliant providers),</p> <code>ResponseResource | Response | Iterator[ResponseStreamEvent]</code> <p>a <code>Response</code> object (non-compliant providers), or an iterator of</p> <code>ResponseResource | Response | Iterator[ResponseStreamEvent]</code> <p><code>ResponseStreamEvent</code> (streaming).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the selected provider does not support the Responses API.</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def responses(\n    model: str,\n    input_data: str | ResponseInputParam,\n    *,\n    provider: str | LLMProvider | None = None,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    max_output_tokens: int | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    stream: bool | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    instructions: str | None = None,\n    max_tool_calls: int | None = None,\n    parallel_tool_calls: int | None = None,\n    reasoning: Any | None = None,\n    text: Any | None = None,\n    presence_penalty: float | None = None,\n    frequency_penalty: float | None = None,\n    truncation: str | None = None,\n    store: bool | None = None,\n    service_tier: str | None = None,\n    user: str | None = None,\n    metadata: dict[str, str] | None = None,\n    previous_response_id: str | None = None,\n    include: list[str] | None = None,\n    background: bool | None = None,\n    safety_identifier: str | None = None,\n    prompt_cache_key: str | None = None,\n    prompt_cache_retention: str | None = None,\n    conversation: str | dict[str, Any] | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; ResponseResource | Response | Iterator[ResponseStreamEvent]:\n    \"\"\"Create a response using the OpenResponses API.\n\n    This implements the OpenResponses specification and returns either\n    `openresponses_types.ResponseResource` (for OpenResponses-compliant providers)\n    or `openai.types.responses.Response` (for providers using OpenAI's native API).\n    If `stream=True`, an iterator of `any_llm.types.responses.ResponseStreamEvent` items is returned.\n\n    Args:\n        model: Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.\n        provider: Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.\n        input_data: The input payload accepted by provider's Responses API.\n            For OpenAI-compatible providers, this is typically a list mixing\n            text, images, and tool instructions, or a dict per OpenAI spec.\n        tools: Optional tools for tool calling (Python callables or OpenAI tool dicts)\n        tool_choice: Controls which tools the model can call\n        max_output_tokens: Maximum number of output tokens to generate\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        stream: Whether to stream response events\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        instructions: A system (or developer) message inserted into the model's context.\n        max_tool_calls: The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.\n        parallel_tool_calls: Whether to allow the model to run tool calls in parallel.\n        reasoning: Configuration options for reasoning models.\n        text: Configuration options for a text response from the model. Can be plain text or structured JSON data.\n        presence_penalty: Penalizes new tokens based on whether they appear in the text so far.\n        frequency_penalty: Penalizes new tokens based on their frequency in the text so far.\n        truncation: Controls how the service truncates input when it exceeds the model context window.\n        store: Whether to store the response so it can be retrieved later.\n        service_tier: The service tier to use for this request.\n        user: A unique identifier representing your end user.\n        metadata: Key-value pairs for custom metadata (up to 16 pairs).\n        previous_response_id: The ID of the response to use as the prior turn for this request.\n        include: Items to include in the response (e.g., 'reasoning.encrypted_content').\n        background: Whether to run the request in the background and return immediately.\n        safety_identifier: A stable identifier used for safety monitoring and abuse detection.\n        prompt_cache_key: A key to use when reading from or writing to the prompt cache.\n        prompt_cache_retention: How long to retain a prompt cache entry created by this request.\n        conversation: The conversation to associate this response with (ID string or ConversationParam object).\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        Either a `ResponseResource` object (OpenResponses-compliant providers),\n        a `Response` object (non-compliant providers), or an iterator of\n        `ResponseStreamEvent` (streaming).\n\n    Raises:\n        NotImplementedError: If the selected provider does not support the Responses API.\n\n    \"\"\"\n    if provider is None:\n        provider_key, model_id = AnyLLM.split_model_provider(model)\n    else:\n        provider_key = LLMProvider.from_string(provider)\n        model_id = model\n\n    llm = AnyLLM.create(\n        provider_key,\n        api_key=api_key,\n        api_base=api_base,\n        **client_args or {},\n    )\n    return llm.responses(\n        model=model_id,\n        input_data=input_data,\n        tools=tools,\n        tool_choice=tool_choice,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        top_p=top_p,\n        stream=stream,\n        instructions=instructions,\n        max_tool_calls=max_tool_calls,\n        parallel_tool_calls=parallel_tool_calls,\n        reasoning=reasoning,\n        text=text,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        truncation=truncation,\n        store=store,\n        service_tier=service_tier,\n        user=user,\n        metadata=metadata,\n        previous_response_id=previous_response_id,\n        include=include,\n        background=background,\n        safety_identifier=safety_identifier,\n        prompt_cache_key=prompt_cache_key,\n        prompt_cache_retention=prompt_cache_retention,\n        conversation=conversation,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/responses/#any_llm.api.aresponses","title":"<code>any_llm.api.aresponses(model, input_data, *, provider=None, tools=None, tool_choice=None, max_output_tokens=None, temperature=None, top_p=None, stream=None, api_key=None, api_base=None, instructions=None, max_tool_calls=None, parallel_tool_calls=None, reasoning=None, text=None, presence_penalty=None, frequency_penalty=None, truncation=None, store=None, service_tier=None, user=None, metadata=None, previous_response_id=None, include=None, background=None, safety_identifier=None, prompt_cache_key=None, prompt_cache_retention=None, conversation=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Create a response using the OpenResponses API.</p> <p>This implements the OpenResponses specification and returns either <code>openresponses_types.ResponseResource</code> (for OpenResponses-compliant providers) or <code>openai.types.responses.Response</code> (for providers using OpenAI's native API). If <code>stream=True</code>, an iterator of <code>any_llm.types.responses.ResponseStreamEvent</code> items is returned.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.</p> required <code>provider</code> <code>str | LLMProvider | None</code> <p>Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.</p> <code>None</code> <code>input_data</code> <code>str | ResponseInputParam</code> <p>The input payload accepted by provider's Responses API. For OpenAI-compatible providers, this is typically a list mixing text, images, and tool instructions, or a dict per OpenAI spec.</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>Optional tools for tool calling (Python callables or OpenAI tool dicts)</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>max_output_tokens</code> <code>int | None</code> <p>Maximum number of output tokens to generate</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream response events</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>instructions</code> <code>str | None</code> <p>A system (or developer) message inserted into the model's context.</p> <code>None</code> <code>max_tool_calls</code> <code>int | None</code> <p>The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.</p> <code>None</code> <code>parallel_tool_calls</code> <code>int | None</code> <p>Whether to allow the model to run tool calls in parallel.</p> <code>None</code> <code>reasoning</code> <code>Any | None</code> <p>Configuration options for reasoning models.</p> <code>None</code> <code>text</code> <code>Any | None</code> <p>Configuration options for a text response from the model. Can be plain text or structured JSON data.</p> <code>None</code> <code>presence_penalty</code> <code>float | None</code> <p>Penalizes new tokens based on whether they appear in the text so far.</p> <code>None</code> <code>frequency_penalty</code> <code>float | None</code> <p>Penalizes new tokens based on their frequency in the text so far.</p> <code>None</code> <code>truncation</code> <code>str | None</code> <p>Controls how the service truncates input when it exceeds the model context window.</p> <code>None</code> <code>store</code> <code>bool | None</code> <p>Whether to store the response so it can be retrieved later.</p> <code>None</code> <code>service_tier</code> <code>str | None</code> <p>The service tier to use for this request.</p> <code>None</code> <code>user</code> <code>str | None</code> <p>A unique identifier representing your end user.</p> <code>None</code> <code>metadata</code> <code>dict[str, str] | None</code> <p>Key-value pairs for custom metadata (up to 16 pairs).</p> <code>None</code> <code>previous_response_id</code> <code>str | None</code> <p>The ID of the response to use as the prior turn for this request.</p> <code>None</code> <code>include</code> <code>list[str] | None</code> <p>Items to include in the response (e.g., 'reasoning.encrypted_content').</p> <code>None</code> <code>background</code> <code>bool | None</code> <p>Whether to run the request in the background and return immediately.</p> <code>None</code> <code>safety_identifier</code> <code>str | None</code> <p>A stable identifier used for safety monitoring and abuse detection.</p> <code>None</code> <code>prompt_cache_key</code> <code>str | None</code> <p>A key to use when reading from or writing to the prompt cache.</p> <code>None</code> <code>prompt_cache_retention</code> <code>str | None</code> <p>How long to retain a prompt cache entry created by this request.</p> <code>None</code> <code>conversation</code> <code>str | dict[str, Any] | None</code> <p>The conversation to associate this response with (ID string or ConversationParam object).</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ResponseResource | Response | AsyncIterator[ResponseStreamEvent]</code> <p>Either a <code>ResponseResource</code> object (OpenResponses-compliant providers),</p> <code>ResponseResource | Response | AsyncIterator[ResponseStreamEvent]</code> <p>a <code>Response</code> object (non-compliant providers), or an iterator of</p> <code>ResponseResource | Response | AsyncIterator[ResponseStreamEvent]</code> <p><code>ResponseStreamEvent</code> (streaming).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the selected provider does not support the Responses API.</p> Source code in <code>src/any_llm/api.py</code> <pre><code>async def aresponses(\n    model: str,\n    input_data: str | ResponseInputParam,\n    *,\n    provider: str | LLMProvider | None = None,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    max_output_tokens: int | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    stream: bool | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    instructions: str | None = None,\n    max_tool_calls: int | None = None,\n    parallel_tool_calls: int | None = None,\n    reasoning: Any | None = None,\n    text: Any | None = None,\n    presence_penalty: float | None = None,\n    frequency_penalty: float | None = None,\n    truncation: str | None = None,\n    store: bool | None = None,\n    service_tier: str | None = None,\n    user: str | None = None,\n    metadata: dict[str, str] | None = None,\n    previous_response_id: str | None = None,\n    include: list[str] | None = None,\n    background: bool | None = None,\n    safety_identifier: str | None = None,\n    prompt_cache_key: str | None = None,\n    prompt_cache_retention: str | None = None,\n    conversation: str | dict[str, Any] | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; ResponseResource | Response | AsyncIterator[ResponseStreamEvent]:\n    \"\"\"Create a response using the OpenResponses API.\n\n    This implements the OpenResponses specification and returns either\n    `openresponses_types.ResponseResource` (for OpenResponses-compliant providers)\n    or `openai.types.responses.Response` (for providers using OpenAI's native API).\n    If `stream=True`, an iterator of `any_llm.types.responses.ResponseStreamEvent` items is returned.\n\n    Args:\n        model: Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.\n        provider: Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.\n        input_data: The input payload accepted by provider's Responses API.\n            For OpenAI-compatible providers, this is typically a list mixing\n            text, images, and tool instructions, or a dict per OpenAI spec.\n        tools: Optional tools for tool calling (Python callables or OpenAI tool dicts)\n        tool_choice: Controls which tools the model can call\n        max_output_tokens: Maximum number of output tokens to generate\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        stream: Whether to stream response events\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        instructions: A system (or developer) message inserted into the model's context.\n        max_tool_calls: The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.\n        parallel_tool_calls: Whether to allow the model to run tool calls in parallel.\n        reasoning: Configuration options for reasoning models.\n        text: Configuration options for a text response from the model. Can be plain text or structured JSON data.\n        presence_penalty: Penalizes new tokens based on whether they appear in the text so far.\n        frequency_penalty: Penalizes new tokens based on their frequency in the text so far.\n        truncation: Controls how the service truncates input when it exceeds the model context window.\n        store: Whether to store the response so it can be retrieved later.\n        service_tier: The service tier to use for this request.\n        user: A unique identifier representing your end user.\n        metadata: Key-value pairs for custom metadata (up to 16 pairs).\n        previous_response_id: The ID of the response to use as the prior turn for this request.\n        include: Items to include in the response (e.g., 'reasoning.encrypted_content').\n        background: Whether to run the request in the background and return immediately.\n        safety_identifier: A stable identifier used for safety monitoring and abuse detection.\n        prompt_cache_key: A key to use when reading from or writing to the prompt cache.\n        prompt_cache_retention: How long to retain a prompt cache entry created by this request.\n        conversation: The conversation to associate this response with (ID string or ConversationParam object).\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        Either a `ResponseResource` object (OpenResponses-compliant providers),\n        a `Response` object (non-compliant providers), or an iterator of\n        `ResponseStreamEvent` (streaming).\n\n    Raises:\n        NotImplementedError: If the selected provider does not support the Responses API.\n\n    \"\"\"\n    if provider is None:\n        provider_key, model_id = AnyLLM.split_model_provider(model)\n    else:\n        provider_key = LLMProvider.from_string(provider)\n        model_id = model\n\n    llm = AnyLLM.create(\n        provider_key,\n        api_key=api_key,\n        api_base=api_base,\n        **client_args or {},\n    )\n    return await llm.aresponses(\n        model=model_id,\n        input_data=input_data,\n        tools=tools,\n        tool_choice=tool_choice,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        top_p=top_p,\n        stream=stream,\n        instructions=instructions,\n        max_tool_calls=max_tool_calls,\n        parallel_tool_calls=parallel_tool_calls,\n        reasoning=reasoning,\n        text=text,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        truncation=truncation,\n        store=store,\n        service_tier=service_tier,\n        user=user,\n        metadata=metadata,\n        previous_response_id=previous_response_id,\n        include=include,\n        background=background,\n        safety_identifier=safety_identifier,\n        prompt_cache_key=prompt_cache_key,\n        prompt_cache_retention=prompt_cache_retention,\n        conversation=conversation,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/types/batch/","title":"Batch","text":""},{"location":"api/types/batch/#batch-types","title":"Batch Types","text":"<p>Data models and types for batch operations.</p>"},{"location":"api/types/batch/#any_llm.types.batch","title":"<code>any_llm.types.batch</code>","text":""},{"location":"api/types/completion/","title":"Completion","text":""},{"location":"api/types/completion/#completion-types","title":"Completion Types","text":"<p>Data models and types for completion operations.</p>"},{"location":"api/types/completion/#any_llm.types.completion","title":"<code>any_llm.types.completion</code>","text":""},{"location":"api/types/completion/#any_llm.types.completion.ChatCompletionMessageFunctionToolCall","title":"<code>ChatCompletionMessageFunctionToolCall</code>","text":"<p>               Bases: <code>ChatCompletionMessageFunctionToolCall</code></p> <p>Extended tool call type that includes extra_content for provider-specific data.</p> <p>The extra_content field is used to store provider-specific metadata that needs to be preserved across multi-turn conversations. For example, Gemini 3 models require thought_signature to be passed back with function calls.</p> Example extra_content structure for Gemini <p>{\"google\": {\"thought_signature\": \"\"}} Source code in <code>src/any_llm/types/completion.py</code> <pre><code>class ChatCompletionMessageFunctionToolCall(OpenAIChatCompletionMessageFunctionToolCall):\n    \"\"\"Extended tool call type that includes extra_content for provider-specific data.\n\n    The extra_content field is used to store provider-specific metadata that needs\n    to be preserved across multi-turn conversations. For example, Gemini 3 models\n    require thought_signature to be passed back with function calls.\n\n    Example extra_content structure for Gemini:\n        {\"google\": {\"thought_signature\": \"&lt;base64-encoded-signature&gt;\"}}\n    \"\"\"\n\n    extra_content: dict[str, Any] | None = None\n</code></pre>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams","title":"<code>CompletionParams</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Normalized parameters for chat completions.</p> <p>This model is used internally to pass structured parameters from the public API layer to provider implementations, avoiding very long function signatures while keeping type safety.</p> Source code in <code>src/any_llm/types/completion.py</code> <pre><code>class CompletionParams(BaseModel):\n    \"\"\"Normalized parameters for chat completions.\n\n    This model is used internally to pass structured parameters from the public\n    API layer to provider implementations, avoiding very long function\n    signatures while keeping type safety.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    model_id: str\n    \"\"\"Model identifier (e.g., 'mistral-small-latest')\"\"\"\n\n    messages: list[dict[str, Any]]\n    \"\"\"List of messages for the conversation\"\"\"\n\n    @field_validator(\"messages\")\n    def check_messages_not_empty(cls, v: list[dict[str, Any]]) -&gt; list[dict[str, Any]]:  # noqa: N805\n        if not v:\n            msg = \"The `messages` list cannot be empty.\"\n            raise ValueError(msg)\n        return v\n\n    tools: list[dict[str, Any] | Any] | None = None\n    \"\"\"List of tools for tool calling. Should be converted to OpenAI tool format dicts\"\"\"\n\n    tool_choice: str | dict[str, Any] | None = None\n    \"\"\"Controls which tools the model can call\"\"\"\n\n    temperature: float | None = None\n    \"\"\"Controls randomness in the response (0.0 to 2.0)\"\"\"\n\n    top_p: float | None = None\n    \"\"\"Controls diversity via nucleus sampling (0.0 to 1.0)\"\"\"\n\n    max_tokens: int | None = None\n    \"\"\"Maximum number of tokens to generate\"\"\"\n\n    response_format: dict[str, Any] | type[BaseModel] | None = None\n    \"\"\"Format specification for the response\"\"\"\n\n    stream: bool | None = None\n    \"\"\"Whether to stream the response\"\"\"\n\n    n: int | None = None\n    \"\"\"Number of completions to generate\"\"\"\n\n    stop: str | list[str] | None = None\n    \"\"\"Stop sequences for generation\"\"\"\n\n    presence_penalty: float | None = None\n    \"\"\"Penalize new tokens based on presence in text\"\"\"\n\n    frequency_penalty: float | None = None\n    \"\"\"Penalize new tokens based on frequency in text\"\"\"\n\n    seed: int | None = None\n    \"\"\"Random seed for reproducible results\"\"\"\n\n    user: str | None = None\n    \"\"\"Unique identifier for the end user\"\"\"\n\n    parallel_tool_calls: bool | None = None\n    \"\"\"Whether to allow parallel tool calls\"\"\"\n\n    logprobs: bool | None = None\n    \"\"\"Include token-level log probabilities in the response\"\"\"\n\n    top_logprobs: int | None = None\n    \"\"\"Number of top alternatives to return when logprobs are requested\"\"\"\n\n    logit_bias: dict[str, float] | None = None\n    \"\"\"Bias the likelihood of specified tokens during generation\"\"\"\n\n    stream_options: dict[str, Any] | None = None\n    \"\"\"Additional options controlling streaming behavior\"\"\"\n\n    max_completion_tokens: int | None = None\n    \"\"\"Maximum number of tokens for the completion (provider-dependent)\"\"\"\n\n    reasoning_effort: ReasoningEffort | None = \"auto\"\n    \"\"\"Reasoning effort level for models that support it. \"auto\" will map to each provider's default.\"\"\"\n</code></pre>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.frequency_penalty","title":"<code>frequency_penalty = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Penalize new tokens based on frequency in text</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.logit_bias","title":"<code>logit_bias = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Bias the likelihood of specified tokens during generation</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.logprobs","title":"<code>logprobs = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Include token-level log probabilities in the response</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.max_completion_tokens","title":"<code>max_completion_tokens = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum number of tokens for the completion (provider-dependent)</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.max_tokens","title":"<code>max_tokens = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum number of tokens to generate</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.messages","title":"<code>messages</code>  <code>instance-attribute</code>","text":"<p>List of messages for the conversation</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.model_id","title":"<code>model_id</code>  <code>instance-attribute</code>","text":"<p>Model identifier (e.g., 'mistral-small-latest')</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.n","title":"<code>n = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of completions to generate</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.parallel_tool_calls","title":"<code>parallel_tool_calls = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to allow parallel tool calls</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.presence_penalty","title":"<code>presence_penalty = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Penalize new tokens based on presence in text</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.reasoning_effort","title":"<code>reasoning_effort = 'auto'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Reasoning effort level for models that support it. \"auto\" will map to each provider's default.</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.response_format","title":"<code>response_format = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Format specification for the response</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.seed","title":"<code>seed = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random seed for reproducible results</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.stop","title":"<code>stop = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Stop sequences for generation</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.stream","title":"<code>stream = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to stream the response</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.stream_options","title":"<code>stream_options = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Additional options controlling streaming behavior</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.temperature","title":"<code>temperature = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Controls randomness in the response (0.0 to 2.0)</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.tool_choice","title":"<code>tool_choice = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Controls which tools the model can call</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.tools","title":"<code>tools = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of tools for tool calling. Should be converted to OpenAI tool format dicts</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.top_logprobs","title":"<code>top_logprobs = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of top alternatives to return when logprobs are requested</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.top_p","title":"<code>top_p = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Controls diversity via nucleus sampling (0.0 to 1.0)</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.user","title":"<code>user = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Unique identifier for the end user</p>"},{"location":"api/types/model/","title":"Model","text":""},{"location":"api/types/model/#model-types","title":"Model Types","text":"<p>Data models and types for model operations.</p>"},{"location":"api/types/model/#any_llm.types.model","title":"<code>any_llm.types.model</code>","text":""},{"location":"api/types/provider/","title":"Provider","text":""},{"location":"api/types/provider/#provider-types","title":"Provider Types","text":"<p>Data models and types for provider operations.</p>"},{"location":"api/types/provider/#any_llm.types.provider","title":"<code>any_llm.types.provider</code>","text":""},{"location":"api/types/provider/#any_llm.types.provider.PlatformKey","title":"<code>PlatformKey</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/any_llm/types/provider.py</code> <pre><code>class PlatformKey(BaseModel):\n    api_key: str\n\n    @field_validator(\"api_key\")\n    @classmethod\n    def validate_api_key_format(cls, value: str) -&gt; str:\n        \"\"\"Validates the API key against the required format.\"\"\"\n        if not re.fullmatch(ANY_API_KEY_REGEX, value):\n            msg = \"Invalid API key format. Must match the pattern: ANY.&lt;version&gt;.&lt;kid&gt;.&lt;fingerprint&gt;-&lt;base64_key&gt;.\"\n            raise ValueError(msg)\n        return value\n</code></pre>"},{"location":"api/types/provider/#any_llm.types.provider.PlatformKey.validate_api_key_format","title":"<code>validate_api_key_format(value)</code>  <code>classmethod</code>","text":"<p>Validates the API key against the required format.</p> Source code in <code>src/any_llm/types/provider.py</code> <pre><code>@field_validator(\"api_key\")\n@classmethod\ndef validate_api_key_format(cls, value: str) -&gt; str:\n    \"\"\"Validates the API key against the required format.\"\"\"\n    if not re.fullmatch(ANY_API_KEY_REGEX, value):\n        msg = \"Invalid API key format. Must match the pattern: ANY.&lt;version&gt;.&lt;kid&gt;.&lt;fingerprint&gt;-&lt;base64_key&gt;.\"\n        raise ValueError(msg)\n    return value\n</code></pre>"},{"location":"api/types/responses/","title":"Responses","text":""},{"location":"api/types/responses/#openresponses-types","title":"OpenResponses Types","text":"<p>Data models and types for the OpenResponses API specification.</p> <p>For the full OpenResponses type definitions, see the openresponses-types package documentation.</p>"},{"location":"api/types/responses/#any_llm.types.responses","title":"<code>any_llm.types.responses</code>","text":""},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams","title":"<code>ResponsesParams</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Normalized parameters for responses API.</p> <p>This model is used internally to pass structured parameters from the public API layer to provider implementations, avoiding very long function signatures while keeping type safety.</p> Source code in <code>src/any_llm/types/responses.py</code> <pre><code>class ResponsesParams(BaseModel):\n    \"\"\"Normalized parameters for responses API.\n\n    This model is used internally to pass structured parameters from the public\n    API layer to provider implementations, avoiding very long function\n    signatures while keeping type safety.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    model: str\n    \"\"\"Model identifier (e.g., 'mistral-small-latest')\"\"\"\n\n    input: str | ResponseInputParam\n    \"\"\"The input payload accepted by provider's Responses API.\n        For OpenAI-compatible providers, this is typically a list mixing\n        text, images, and tool instructions, or a dict per OpenAI spec.\n    \"\"\"\n\n    instructions: str | None = None\n\n    max_tool_calls: int | None = None\n\n    text: Any | None = None\n\n    tools: list[dict[str, Any]] | None = None\n    \"\"\"List of tools for tool calling. Should be converted to OpenAI tool format dicts\"\"\"\n\n    tool_choice: str | dict[str, Any] | None = None\n    \"\"\"Controls which tools the model can call\"\"\"\n\n    temperature: float | None = None\n    \"\"\"Controls randomness in the response (0.0 to 2.0)\"\"\"\n\n    top_p: float | None = None\n    \"\"\"Controls diversity via nucleus sampling (0.0 to 1.0)\"\"\"\n\n    max_output_tokens: int | None = None\n    \"\"\"Maximum number of tokens to generate\"\"\"\n\n    response_format: dict[str, Any] | type[BaseModel] | None = None\n    \"\"\"Format specification for the response\"\"\"\n\n    stream: bool | None = None\n    \"\"\"Whether to stream the response\"\"\"\n\n    parallel_tool_calls: bool | None = None\n    \"\"\"Whether to allow parallel tool calls\"\"\"\n\n    top_logprobs: int | None = None\n    \"\"\"Number of top alternatives to return when logprobs are requested\"\"\"\n\n    stream_options: dict[str, Any] | None = None\n    \"\"\"Additional options controlling streaming behavior\"\"\"\n\n    reasoning: dict[str, Any] | None = None\n    \"\"\"Configuration options for reasoning models.\"\"\"\n\n    presence_penalty: float | None = None\n    \"\"\"Penalizes new tokens based on whether they appear in the text so far.\"\"\"\n\n    frequency_penalty: float | None = None\n    \"\"\"Penalizes new tokens based on their frequency in the text so far.\"\"\"\n\n    truncation: str | None = None\n    \"\"\"Controls how the service truncates the input when it exceeds the model context window.\"\"\"\n\n    store: bool | None = None\n    \"\"\"Whether to store the response so it can be retrieved later.\"\"\"\n\n    service_tier: str | None = None\n    \"\"\"The service tier to use for this request.\"\"\"\n\n    user: str | None = None\n    \"\"\"A unique identifier representing your end user.\"\"\"\n\n    metadata: dict[str, str] | None = None\n    \"\"\"Key-value pairs for custom metadata (up to 16 pairs).\"\"\"\n\n    previous_response_id: str | None = None\n    \"\"\"The ID of the response to use as the prior turn for this request.\"\"\"\n\n    include: list[str] | None = None\n    \"\"\"Items to include in the response (e.g., 'reasoning.encrypted_content').\"\"\"\n\n    background: bool | None = None\n    \"\"\"Whether to run the request in the background and return immediately.\"\"\"\n\n    safety_identifier: str | None = None\n    \"\"\"A stable identifier used for safety monitoring and abuse detection.\"\"\"\n\n    prompt_cache_key: str | None = None\n    \"\"\"A key to use when reading from or writing to the prompt cache.\"\"\"\n\n    prompt_cache_retention: str | None = None\n    \"\"\"How long to retain a prompt cache entry created by this request.\"\"\"\n\n    conversation: str | dict[str, Any] | None = None\n    \"\"\"The conversation to associate this response with (ID string or ConversationParam object).\"\"\"\n</code></pre>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.background","title":"<code>background = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to run the request in the background and return immediately.</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.conversation","title":"<code>conversation = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The conversation to associate this response with (ID string or ConversationParam object).</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.frequency_penalty","title":"<code>frequency_penalty = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Penalizes new tokens based on their frequency in the text so far.</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.include","title":"<code>include = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Items to include in the response (e.g., 'reasoning.encrypted_content').</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.input","title":"<code>input</code>  <code>instance-attribute</code>","text":"<p>The input payload accepted by provider's Responses API. For OpenAI-compatible providers, this is typically a list mixing text, images, and tool instructions, or a dict per OpenAI spec.</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.max_output_tokens","title":"<code>max_output_tokens = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum number of tokens to generate</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.metadata","title":"<code>metadata = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Key-value pairs for custom metadata (up to 16 pairs).</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.model","title":"<code>model</code>  <code>instance-attribute</code>","text":"<p>Model identifier (e.g., 'mistral-small-latest')</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.parallel_tool_calls","title":"<code>parallel_tool_calls = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to allow parallel tool calls</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.presence_penalty","title":"<code>presence_penalty = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Penalizes new tokens based on whether they appear in the text so far.</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.previous_response_id","title":"<code>previous_response_id = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The ID of the response to use as the prior turn for this request.</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.prompt_cache_key","title":"<code>prompt_cache_key = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A key to use when reading from or writing to the prompt cache.</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.prompt_cache_retention","title":"<code>prompt_cache_retention = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How long to retain a prompt cache entry created by this request.</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.reasoning","title":"<code>reasoning = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Configuration options for reasoning models.</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.response_format","title":"<code>response_format = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Format specification for the response</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.safety_identifier","title":"<code>safety_identifier = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A stable identifier used for safety monitoring and abuse detection.</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.service_tier","title":"<code>service_tier = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The service tier to use for this request.</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.store","title":"<code>store = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to store the response so it can be retrieved later.</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.stream","title":"<code>stream = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to stream the response</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.stream_options","title":"<code>stream_options = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Additional options controlling streaming behavior</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.temperature","title":"<code>temperature = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Controls randomness in the response (0.0 to 2.0)</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.tool_choice","title":"<code>tool_choice = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Controls which tools the model can call</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.tools","title":"<code>tools = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of tools for tool calling. Should be converted to OpenAI tool format dicts</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.top_logprobs","title":"<code>top_logprobs = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of top alternatives to return when logprobs are requested</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.top_p","title":"<code>top_p = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Controls diversity via nucleus sampling (0.0 to 1.0)</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.truncation","title":"<code>truncation = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Controls how the service truncates the input when it exceeds the model context window.</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.user","title":"<code>user = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A unique identifier representing your end user.</p>"},{"location":"cookbooks/any_llm_getting_started/","title":"Getting Started with Any-LLM","text":"<p>Any-LLM is a unified interface that lets you work with language models from any provider using a consistent API. Whether you're using OpenAI, Anthropic, Google, local models, or open-source alternatives, any-llm makes it easy to switch between them without changing your code.</p> In\u00a0[\u00a0]: Copied! <pre>%pip install any-llm-sdk[all] nest-asyncio -q\n\n# nest_asyncio allows us to use 'await' directly in Jupyter notebooks\n# This is needed because any-llm uses async functions for API calls\nimport nest_asyncio\n\nnest_asyncio.apply()\n</pre> %pip install any-llm-sdk[all] nest-asyncio -q  # nest_asyncio allows us to use 'await' directly in Jupyter notebooks # This is needed because any-llm uses async functions for API calls import nest_asyncio  nest_asyncio.apply() In\u00a0[\u00a0]: Copied! <pre>import os\nfrom getpass import getpass\n\n\ndef setup_api_key(key_name: str, provider: str) -&gt; None:\n    \"\"\"Set up API key for the specified provider.\"\"\"\n    if key_name not in os.environ:\n        print(f\"\ud83d\udd11 {key_name} not found in environment\")\n        api_key = getpass(f\"Enter your {provider} API key (or press Enter to skip): \")\n        if api_key:\n            os.environ[key_name] = api_key\n            print(f\"\u2705 {key_name} set for this session\")\n        else:\n            print(f\"\u23ed\ufe0f  Skipping {provider}\")\n    else:\n        print(f\"\u2705 {key_name} found in environment\")\n\n\n# Set up keys for different providers\nprint(\"Setting up API keys...\\n\")\nsetup_api_key(\"OPENAI_API_KEY\", \"OpenAI\")\nsetup_api_key(\"ANTHROPIC_API_KEY\", \"Anthropic\")\n\n#  You could add more using :\n# setup_api_key(\"GOOGLE_API_KEY\", \"Google\")\n# setup_api_key(\"MISTRAL_API_KEY\", \"Mistral\")\n</pre> import os from getpass import getpass   def setup_api_key(key_name: str, provider: str) -&gt; None:     \"\"\"Set up API key for the specified provider.\"\"\"     if key_name not in os.environ:         print(f\"\ud83d\udd11 {key_name} not found in environment\")         api_key = getpass(f\"Enter your {provider} API key (or press Enter to skip): \")         if api_key:             os.environ[key_name] = api_key             print(f\"\u2705 {key_name} set for this session\")         else:             print(f\"\u23ed\ufe0f  Skipping {provider}\")     else:         print(f\"\u2705 {key_name} found in environment\")   # Set up keys for different providers print(\"Setting up API keys...\\n\") setup_api_key(\"OPENAI_API_KEY\", \"OpenAI\") setup_api_key(\"ANTHROPIC_API_KEY\", \"Anthropic\")  #  You could add more using : # setup_api_key(\"GOOGLE_API_KEY\", \"Google\") # setup_api_key(\"MISTRAL_API_KEY\", \"Mistral\") In\u00a0[\u00a0]: Copied! <pre>from any_llm import AnyLLM, LLMProvider\n\nfor provider in [LLMProvider.OPENAI, LLMProvider.ANTHROPIC]:\n    client = AnyLLM.create(provider=provider)\n    models = client.list_models()\n    print(f\"Provider: {provider}\")\n    print(\", \".join([model.id for model in models]))\n    print()\n</pre> from any_llm import AnyLLM, LLMProvider  for provider in [LLMProvider.OPENAI, LLMProvider.ANTHROPIC]:     client = AnyLLM.create(provider=provider)     models = client.list_models()     print(f\"Provider: {provider}\")     print(\", \".join([model.id for model in models]))     print() In\u00a0[\u00a0]: Copied! <pre>from any_llm import acompletion\nfrom any_llm.types.completion import ChatCompletion\n\nprompt = \"Write a Haiku on the solar system.\"\n\n# OpenAI\nmodel = \"openai:gpt-4o-mini\"\nresult = await acompletion(\n    model=model,\n    messages=[\n        {\"role\": \"user\", \"content\": prompt},\n    ],\n)\nassert isinstance(result, ChatCompletion)\n\nprint(f\"Model: {result.model}\")\nprint(f\"Response:\\n{result.choices[0].message.content}\\n\")\n\n# Anthropic\nmodel = \"anthropic:claude-haiku-4-5-20251001\"\nresult = await acompletion(\n    model=model,\n    messages=[\n        {\"role\": \"user\", \"content\": prompt},\n    ],\n)\n\nassert isinstance(result, ChatCompletion)\n\nprint(f\"Model: {result.model}\")\nprint(f\"Response:\\n{result.choices[0].message.content}\")\n</pre> from any_llm import acompletion from any_llm.types.completion import ChatCompletion  prompt = \"Write a Haiku on the solar system.\"  # OpenAI model = \"openai:gpt-4o-mini\" result = await acompletion(     model=model,     messages=[         {\"role\": \"user\", \"content\": prompt},     ], ) assert isinstance(result, ChatCompletion)  print(f\"Model: {result.model}\") print(f\"Response:\\n{result.choices[0].message.content}\\n\")  # Anthropic model = \"anthropic:claude-haiku-4-5-20251001\" result = await acompletion(     model=model,     messages=[         {\"role\": \"user\", \"content\": prompt},     ], )  assert isinstance(result, ChatCompletion)  print(f\"Model: {result.model}\") print(f\"Response:\\n{result.choices[0].message.content}\")"},{"location":"cookbooks/any_llm_getting_started/#getting-started-with-any-llm","title":"Getting Started with Any-LLM\u00b6","text":""},{"location":"cookbooks/any_llm_getting_started/#why-any-llm","title":"Why Any-LLM?\u00b6","text":"<ul> <li>Provider Agnostic: One API for all LLM providers</li> <li>Easy Switching: Change models with a single line</li> <li>Cost Comparison: Compare costs across providers</li> <li>Streaming Support: Real-time responses from any model</li> <li>Type Safe: Full TypeScript/Python type support</li> </ul>"},{"location":"cookbooks/any_llm_getting_started/#installation","title":"Installation\u00b6","text":""},{"location":"cookbooks/any_llm_getting_started/#setting-up-api-keys","title":"Setting Up API Keys\u00b6","text":"<p>Different providers require different API keys. Let's set them up properly:</p>"},{"location":"cookbooks/any_llm_getting_started/#list-models-across-providers","title":"List Models Across Providers\u00b6","text":"<p><code>any_llm</code> can list all available models for an LLM provider - in this case, we are listing out models supported by OpenAI and Anthropic.</p>"},{"location":"cookbooks/any_llm_getting_started/#expected-output","title":"Expected output\u00b6","text":"<p>Provider: openai gpt-4o-mini, gpt-4-0613, gpt-4, gpt-3.5-turbo, gpt-5-search-api-2025-10-14, gpt-realtime-mini, gpt-realtime-mini-2025-10-06, sora-2, sora-2-pro, davinci-002, babbage-002, gpt-3.5-turbo-instruct, gpt-3.5-turbo-instruct-0914...</p> <p>Provider: anthropic claude-haiku-4-5-20251001, claude-sonnet-4-5-20250929, claude-opus-4-1-20250805, claude-opus-4-20250514, claude-sonnet-4-20250514, claude-3-7-sonnet-20250219, claude-3-5-haiku-20241022, claude-3-haiku-20240307</p>"},{"location":"cookbooks/any_llm_getting_started/#generate-text","title":"Generate Text\u00b6","text":"<p>Let's use one model from each provider to generate text for the same prompt.</p>"},{"location":"cookbooks/any_llm_getting_started/#expected-output","title":"Expected Output\u00b6","text":"<p>Note: The haiku content will be different each time since it's generated by the LLM. This example shows the output format.</p> <p>Model: gpt-4o-mini-2024-07-18 Response:</p> <p>Planets spin and dance, In the vast cosmic embrace, Stars whisper their tales.</p> <p>Model: claude-haiku-4-5-20251001 Response:</p> <p>Eight worlds circle round, Sun's gravity holds them close\u2014 Dance through endless void.</p>"},{"location":"gateway/api-reference/","title":"API Reference","text":""},{"location":"gateway/authentication/","title":"Authentication","text":"<p>any-llm-gateway offers two authentication methods, each designed for different use cases. Understanding when to use each approach will help you secure your gateway effectively.</p>"},{"location":"gateway/authentication/#authentication-methods-overview","title":"Authentication Methods Overview","text":"Method Best For Key Management Usage Tracking Master Key Internal services, admin operations, trusted environments Single key with full access Requires manual user specification Virtual API Keys External apps, per-user access, customer-facing services Multiple scoped keys Automatic per-key tracking"},{"location":"gateway/authentication/#supported-headers","title":"Supported Headers","text":"<p>The gateway accepts authentication via two headers:</p> <ul> <li><code>X-AnyLLM-Key</code> (preferred): The gateway's native authentication header</li> <li><code>Authorization</code>: Standard HTTP authorization header for OpenAI client compatibility</li> </ul> <p>Both headers use the <code>Bearer &lt;token&gt;</code> format. When both headers are present, <code>X-AnyLLM-Key</code> takes precedence.</p> <p>Using the <code>Authorization</code> header allows you to use the gateway with OpenAI-compatible clients without modification:</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://localhost:8000/v1\",\n    api_key=\"your-master-key-or-virtual-key\",  # Sent as Authorization: Bearer ...\n)\n</code></pre>"},{"location":"gateway/authentication/#master-key","title":"Master Key","text":"<p>The master key is the root credential for your gateway installation. It has unrestricted access to all gateway operations and should be treated with the same security as your production database credentials.</p>"},{"location":"gateway/authentication/#generating-a-master-key","title":"Generating a Master Key","text":"<p>Generate a cryptographically secure master key (minimum 32 characters recommended):</p> <pre><code>python -c \"import secrets; print(secrets.token_urlsafe(32))\"\n</code></pre> <p>Example output: <pre><code>Zx8Q_vKm3nR7wP2sT9yU5iO1eA6hD4fG0bN8cL3jM5k\n</code></pre></p> <p>Set the generated key in your configuration:</p> <p>Using environment variables: <pre><code>export GATEWAY_MASTER_KEY=\"Zx8Q_vKm3nR7wP2sT9yU5iO1eA6hD4fG0bN8cL3jM5k\"\n</code></pre></p> <p>Using config.yml: <pre><code>master_key: \"Zx8Q_vKm3nR7wP2sT9yU5iO1eA6hD4fG0bN8cL3jM5k\"\n</code></pre></p>"},{"location":"gateway/authentication/#creating-a-user","title":"Creating a User","text":"<pre><code>curl -X POST http://localhost:8000/v1/users \\\n  -H \"X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"user_id\": \"user-123\", \"alias\": \"Alice\"}'\n</code></pre> With optional metadata <pre><code>curl -X POST http://localhost:8000/v1/users \\\n  -H \"X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d ' { \n    \"user_id\": \"user-123\", \n    \"alias\": \"Alice\",\n    \"metadata\": {\n      \"department\": \"Engineering\",\n      \"team\": \"ML\",\n      \"email\": \"alice@example.com\"\n    }\n  }'\n</code></pre>"},{"location":"gateway/authentication/#making-requests-with-master-key","title":"Making Requests with Master Key","text":"<p>When using the master key, you must specify which user is making the request using the <code>user</code> field:</p> <p><pre><code>curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openai:gpt-4o-mini\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Write a haiku on Jupiter\"}],\n    \"user\": \"user-123\"\n  }'\n</code></pre> The <code>user</code> field tells the gateway which user's budget and spend tracking to update. Without this field, the request will be rejected.</p>"},{"location":"gateway/authentication/#virtual-api-keys","title":"Virtual API Keys","text":"<p>Virtual API keys provide scoped access for making completion requests without exposing the master key. Each virtual key can have expiration dates, metadata, and associated users for automatic usage tracking.</p>"},{"location":"gateway/authentication/#creating-a-virtual-api-key","title":"Creating a Virtual API Key","text":"<p>Create a virtual key with a descriptive name : </p> <pre><code>curl -X POST http://localhost:8000/v1/keys \\\n  -H \"X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"key_name\": \"mobile-app\"}'\n</code></pre> <p>Important: Save the <code>key</code> value immediately\u2014it's only shown once and cannot be retrieved later.</p> Example Response <pre><code>{\n  \"id\": \"abc-123\",\n  \"key\": \"gw-...\",\n  \"key_name\": \"mobile-app\",\n  \"created_at\": \"2025-10-20T10:00:00\",\n  \"expires_at\": null,\n  \"is_active\": true,\n  \"metadata\": {}\n}\n</code></pre>"},{"location":"gateway/authentication/#key-with-expiration","title":"Key with Expiration","text":"<p>Create a key that automatically expires on a specific date:</p> <pre><code>curl -X POST http://localhost:8000/v1/keys \\\n  -H \"X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"key_name\": \"trial-access\",\n    \"expires_at\": \"2025-12-31T23:59:59Z\"\n  }'\n</code></pre>"},{"location":"gateway/authentication/#using-virtual-api-keys","title":"Using Virtual API Keys","text":"<p>Making requests with a virtual key is simpler than using the master key\u2014no <code>user</code> field is required:</p> <pre><code>curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"X-AnyLLM-Key: Bearer gw-...\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"openai:gpt-5-mini\", \"messages\": [{\"role\": \"user\", \"content\": \"Write a haiku on Saturn\"}]}'\n</code></pre> <p>The gateway automatically tracks usage based on the virtual key used.</p>"},{"location":"gateway/authentication/#managing-virtual-keys","title":"Managing Virtual Keys","text":""},{"location":"gateway/authentication/#list-all-keys","title":"List All Keys","text":"<p>List all keys: <pre><code>curl http://localhost:8000/v1/keys \\\n  -H \"X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}\"\n</code></pre></p> <p>Deactivate a key: <pre><code>curl -X PATCH http://localhost:8000/v1/keys/&lt;virtual_key_id&gt;\\\n  -H \"X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"is_active\": false}'\n</code></pre></p> <p>Delete a key: <pre><code>curl -X DELETE http://localhost:8000/v1/keys/&lt;virtual_key_id&gt; \\\n  -H \"X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}\"\n</code></pre></p> <p>See API Reference for complete key management operations.</p> <p>Note: The actual key values are never returned in list or get operations for security reasons.</p>"},{"location":"gateway/authentication/#next-steps","title":"Next Steps","text":"<p>Now that you understand authentication, explore these related topics:</p> <ul> <li>Budget Management - Set spending limits for users and enforce budgets</li> <li>Configuration - Learn about provider setup and pricing configuration</li> <li>API Reference - Explore all available endpoints for managing keys and users</li> <li>Quick Start - Complete walkthrough of setting up your first gateway</li> </ul> <p>For questions or issues, refer to the troubleshooting guide or check the project's issue tracker.</p>"},{"location":"gateway/budget-management/","title":"Budget Management","text":"<p>Budgets provide shared spending limits that can be assigned to multiple users. This allows you to create budget tiers (like \"Free\", \"Pro\", \"Enterprise\") and enforce spending limits across groups of users.</p>"},{"location":"gateway/budget-management/#creating-a-budget","title":"Creating a Budget","text":"<pre><code># Create a budget with a $10.00 spending limit and monthly resets (30 days = 2592000 seconds)\ncurl -X POST http://localhost:8000/v1/budgets \\\n  -H \"X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"max_budget\": 10.0,\n    \"budget_duration_sec\": 2592000\n  }'\n</code></pre>  Sample Response <pre><code>{\n  \"budget_id\": \"abc-123\",\n  \"max_budget\": 10.0,\n  \"budget_duration_sec\": 2592000,\n  \"created_at\": \"2025-10-22T10:00:00Z\",\n  \"updated_at\": \"2025-10-22T10:00:00Z\"\n}\n</code></pre>"},{"location":"gateway/budget-management/#assigning-budgets-to-users","title":"Assigning Budgets to Users","text":"<p>When creating or updating a user, specify the <code>budget_id</code>:</p> <p>Warning: If you don't create and set a budget, budget is unlimited</p> <pre><code># Create a user with a budget\ncurl -X POST http://localhost:8000/v1/users \\\n  -H \"X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"user_id\": \"user-456\",\n    \"alias\": \"Bob\",\n    \"budget_id\": \"abc-123\"\n  }'\n\n# Update an existing user's budget\ncurl -X PATCH http://localhost:8000/v1/users/user-123 \\\n  -H \"X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"budget_id\": \"abc-123\"}'\n</code></pre>"},{"location":"gateway/budget-management/#per-user-budget-resets","title":"Per-User Budget Resets","text":"<p>Budget resets are per-user, not global. Each user tracks their own budget period based on when they were assigned the budget.</p> <p>Example: 1. Create a budget with <code>budget_duration_sec: 604800</code> (1 week) 2. Assign User A to the budget on Monday 3. Assign User B to the budget on Tuesday 4. User A's budget resets every Monday 5. User B's budget resets every Tuesday</p> <p>This allows you to create budget tiers (like \"Free\", \"Pro\", \"Enterprise\") without worrying about all users resetting at the same time.</p>"},{"location":"gateway/budget-management/#automatic-reset-behavior","title":"Automatic Reset Behavior","text":"<p>Budget resets happen automatically using a \"lazy reset\" approach: - When a user makes a request, the system checks if their <code>next_budget_reset_at</code> has passed - If yes, the user's <code>spend</code> is reset to $0.00 and a new reset date is calculated - A log entry is created in <code>budget_reset_logs</code> for audit purposes - The request then proceeds normally</p>"},{"location":"gateway/configuration/","title":"Configuration","text":"<p>The any-llm-gateway requires configuration to connect to your database, authenticate requests, and route to LLM providers. This guide covers the two main configuration approaches and how to set up model pricing for cost tracking.</p> <p>You can configure the gateway using either a YAML configuration file or environment variables:</p> <ul> <li>Config File (Recommended): Best for development and when managing multiple providers with complex settings. Easier to version control and share across teams.</li> <li>Environment Variables: Best for production deployments, containerized environments, or when following 12-factor app principles.</li> </ul> <p>Both methods can also be combined\u2014environment variables will override config file values.</p>"},{"location":"gateway/configuration/#option-1-config-file","title":"Option 1: Config File","text":"<p>Create a <code>config.yml</code> file with your database connection, master key, and provider credentials:</p> <p>Generating a secure master key: <pre><code> python -c \"import secrets; print(secrets.token_urlsafe(32))\"\n</code></pre></p> <pre><code>#Database connection\ndatabase_url: \"postgresql://gateway:gateway@localhost:5432/gateway_db\"\n\n#Master key for admin access\nmaster_key: \"your-secure-master-key\"\n\n## LLM Provider Credentials\nproviders:\n  openai:\n    api_key: \"${OPENAI_API_KEY}\"\n  gemini:\n    api_key: \"${GEMINI_API_KEY}\"\n  vertexai:\n    credentials: \"/path/to/service_account.json\"\n    project: \"your-gcp-project-id\"\n    location: \"us-central1\"\n\n# Model pricing for cost-tracking (optional)\npricing:\n  openai:gpt-4:\n    input_price_per_million: 0.15\n    output_price_per_million: 0.6\n</code></pre> <p>Start the gateway with your config file:</p> <pre><code>any-llm-gateway serve --config config.yml\n</code></pre>"},{"location":"gateway/configuration/#option-2-environment-variables","title":"Option 2: Environment Variables","text":"<p>Configure the gateway entirely through environment variables\u2014useful for containerized deployments:</p> <pre><code>#Required settings\nexport DATABASE_URL=\"postgresql://gateway:gateway@localhost:5432/gateway_db\"\nexport GATEWAY_MASTER_KEY=\"your-secure-master-key\"\nexport GATEWAY_HOST=\"0.0.0.0\"\nexport GATEWAY_PORT=8000\n\nany-llm-gateway serve\n</code></pre> <p>Note: Model pricing cannot be set via environment variables. Use the config file or the Pricing API instead.</p>"},{"location":"gateway/configuration/#model-pricing-configuration","title":"Model Pricing Configuration","text":"<p>Configure model pricing in your config file to automatically track costs. Pricing can be set via config file or dynamically via the API.</p>"},{"location":"gateway/configuration/#config-file-pricing","title":"Config File Pricing","text":"<p>Add pricing for models in your config file using the format <code>provider:model-name</code>:</p> <pre><code>pricing:\n  openai:gpt-3.5-turbo:\n    input_price_per_million: 0.5\n    output_price_per_million: 1.5\n</code></pre>"},{"location":"gateway/configuration/#dynamic-pricing-via-api","title":"Dynamic Pricing via API","text":"<p>You can also set or update pricing dynamically using the API: <pre><code>curl -X POST http://localhost:8000/v1/pricing \\\n  -H \"X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openai:gpt-4\",\n    \"input_price_per_million\": 30.0,\n    \"output_price_per_million\": 60.0\n  }'\n</code></pre></p> <p>This is useful for: - Updating pricing without restarting the gateway - Managing pricing in production environments - Adjusting rates as provider pricing changes</p> <p>Important notes: - Database pricing takes precedence - config only sets initial values - If pricing for the model already exists in the database, config values are ignored (with a warning logged)</p>"},{"location":"gateway/configuration/#provider-client-args","title":"Provider Client Args","text":"<p>You can set additional arguments to provider clients via the <code>client_args</code> configuration. These arguments are passed directly to the provider's client initialization, enabling custom headers, timeouts, and other provider-specific options.</p> <pre><code>providers:\n  openai:\n    api_key: \"${OPENAI_API_KEY}\"\n    client_args:\n      custom_headers:\n        X-Custom-Header: \"custom-value\"\n      timeout: 60\n</code></pre> <p>Common use cases: - Custom headers: Pass additional headers to the provider (e.g., for proxy authentication or request tracing) - Timeouts: Configure connection and request timeouts - Provider-specific options: Pass any additional arguments supported by the provider's client</p> <p>The available <code>client_args</code> options depend on the provider. See the any-llm provider documentation for provider-specific options.</p>"},{"location":"gateway/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>See supported providers for provider-specific configuration</li> <li>Learn about authentication methods for managing access</li> <li>Set up budget management to enforce spending limits</li> </ul>"},{"location":"gateway/docker-deployment/","title":"Docker Deployment Guide","text":"<p>This guide walks you through deploying <code>any-llm-gateway</code> using Docker and Docker Compose. Whether you're setting up a local development environment or deploying to production, this guide covers the essential steps and best practices for a secure, reliable deployment.</p>"},{"location":"gateway/docker-deployment/#quick-start-with-docker-compose","title":"Quick Start with Docker Compose","text":"<p>Docker Compose is the recommended deployment method for most users. It automatically sets up both the gateway application and a PostgreSQL database with proper networking and dependencies.</p> <p>Prerequisites: - Docker Engine 20.10 or newer - Docker Compose 2.0 or newer - At least one LLM provider API key (OpenAI, Anthropic, Mistral, etc.)</p>"},{"location":"gateway/docker-deployment/#configure-the-gateway","title":"Configure the Gateway","text":"<p>First, prepare your configuration file with credentials and settings:</p> <p>Copy the example configuration file:</p> <pre><code>cp docker/config.example.yml docker/config.yml\n</code></pre> <p>Generate a secure master key (minimum 32 characters recommended):</p> <pre><code>python -c \"import secrets; print(secrets.token_urlsafe(32))\"\n</code></pre> <p>Save the output of this command for the next step. Learn more about keys here.</p> <p>Edit <code>docker/config.yml</code> with your master key and provider credentials. See the Configuration Guide for detailed options.</p>"},{"location":"gateway/docker-deployment/#start-the-services","title":"Start the Services","text":"<p>Launch the gateway and database with a single command:</p> <pre><code>docker-compose -f docker/docker-compose.yml up -d\n</code></pre> <p>This command will: - Pull the PostgreSQL 16 Alpine image - Build the gateway Docker image from source (or pull from GHCR if configured) - Create a dedicated network for service communication - Start PostgreSQL with automatic health checks - Wait for the database to be healthy before starting the gateway - Initialize database tables and schema automatically</p> <p>The <code>-d</code> flag runs services in detached mode (background).</p>"},{"location":"gateway/docker-deployment/#verify-deployment","title":"Verify Deployment","text":"<p>Confirm everything is running correctly:</p> <pre><code># Test the health endpoint\ncurl http://localhost:8000/health\n# Expected: {\"status\": \"healthy\"}\n\n# Check service status\ndocker-compose -f docker/docker-compose.yml ps\n\n# View real-time logs\ndocker-compose -f docker/docker-compose.yml logs -f gateway\n</code></pre> <p>If the health check returns successfully, your gateway is ready to accept requests!</p>"},{"location":"gateway/docker-deployment/#standalone-docker-deployment","title":"Standalone Docker Deployment","text":"<p>For scenarios where you have an existing PostgreSQL database or prefer more control over your deployment architecture, you can run the gateway as a standalone container.</p>"},{"location":"gateway/docker-deployment/#using-pre-built-image","title":"Using Pre-built Image","text":"<p>Pull and run the official image from GitHub Container Registry:</p> <pre><code>docker pull ghcr.io/mozilla-ai/any-llm/gateway:latest\n\ndocker run -d \\\n  --name any-llm-gateway \\\n  -p 8000:8000 \\\n  -v $(pwd)/config.yml:/app/config.yml \\\n  -e DATABASE_URL=\"postgresql://user:pass@host:5432/dbname\" \\\n  ghcr.io/mozilla-ai/any-llm/gateway:latest \\\n  any-llm-gateway serve --config /app/config.yml\n</code></pre> <p>Replace the <code>DATABASE_URL</code> with your actual PostgreSQL connection string. The format is: <code>postgresql://username:password@hostname:port/database_name</code></p>"},{"location":"gateway/docker-deployment/#building-from-source","title":"Building from Source","text":"<p>If you need to customize the image or test local changes:</p> <pre><code>docker build -t any-llm-gateway:local -f docker/Dockerfile .\n\ndocker run -d \\\n  --name any-llm-gateway \\\n  -p 8000:8000 \\\n  -v $(pwd)/config.yml:/app/config.yml \\\n  -e DATABASE_URL=\"postgresql://user:pass@host:5432/dbname\" \\\n  any-llm-gateway:local\n</code></pre>"},{"location":"gateway/docker-deployment/#production-deployment","title":"Production Deployment","text":"<p>Production deployments require additional considerations for reliability, security, and performance.</p>"},{"location":"gateway/docker-deployment/#production-configuration","title":"Production Configuration","text":"<p>Enhance your docker-compose.yml with production-grade settings:</p> <pre><code>services:\n  gateway:\n    image: ghcr.io/mozilla-ai/any-llm/gateway:latest\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"python\", \"-c\", \"import urllib.request; urllib.request.urlopen('http://localhost:8000/health')\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 40s\n    deploy:\n      resources:\n        limits:\n          cpus: '2'\n          memory: 2G\n        reservations:\n          cpus: '1'\n          memory: 1G\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n</code></pre>"},{"location":"gateway/docker-deployment/#nginx-reverse-proxy","title":"Nginx Reverse Proxy","text":"<p>For production, always use a reverse proxy with HTTPS:</p> <pre><code>server {\n    listen 443 ssl http2;\n    server_name gateway.yourdomain.com;\n\n    ssl_certificate /etc/ssl/certs/gateway.crt;\n    ssl_certificate_key /etc/ssl/private/gateway.key;\n\n    # Security headers\n    add_header Strict-Transport-Security \"max-age=31536000\" always;\n\n    location / {\n        proxy_pass http://localhost:8000;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n\n        # Timeouts for LLM streaming\n        proxy_read_timeout 300s;\n        proxy_connect_timeout 75s;\n    }\n}\n</code></pre>"},{"location":"gateway/docker-deployment/#environment-variables","title":"Environment Variables","text":"<p>The gateway can be configured using environment variables instead of or in addition to a config file. This is useful for Docker deployments and follows 12-factor app principles.</p> <p>For a complete list of environment variables and configuration options, see the Configuration Guide.</p> <p>Docker Compose example with .env file:</p> <pre><code>services:\n  gateway:\n    env_file:\n      - .env\n</code></pre>"},{"location":"gateway/docker-deployment/#database-backups","title":"Database Backups","text":"<pre><code># Backup\ndocker-compose -f docker/docker-compose.yml exec postgres \\\n  pg_dump -U gateway gateway &gt; backup.sql\n\n# Restore\ndocker-compose -f docker/docker-compose.yml exec -T postgres \\\n  psql -U gateway gateway &lt; backup.sql\n</code></pre>"},{"location":"gateway/docker-deployment/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Never commit secrets - Use <code>.env</code> files (gitignored) or Docker secrets</li> <li>Use read-only volumes - Mount configs with <code>:ro</code> flag</li> <li>Enable HTTPS - Use a reverse proxy with SSL certificates</li> <li>Isolate networks - Keep database on internal network only</li> <li>Update regularly - Use tagged versions and update containers periodically</li> </ol>"},{"location":"gateway/docker-deployment/#monitoring-and-logging","title":"Monitoring and Logging","text":""},{"location":"gateway/docker-deployment/#health-checks","title":"Health Checks","text":"<pre><code># Test health endpoint\ncurl http://localhost:8000/health\n\n# Check container health status\ndocker inspect --format='{{.State.Health.Status}}' container-name\n</code></pre>"},{"location":"gateway/docker-deployment/#logging","title":"Logging","text":"<pre><code># View logs\ndocker-compose logs -f gateway\n\n# Last 100 lines\ndocker-compose logs --tail=100 gateway\n</code></pre> <p>Configure log rotation:</p> <pre><code>services:\n  gateway:\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n</code></pre>"},{"location":"gateway/docker-deployment/#troubleshooting","title":"Troubleshooting","text":"<p>Container won't start: <pre><code>docker-compose logs gateway\n</code></pre> Common issues: Database connection failed, port in use, missing config</p> <p>Database connection issues: <pre><code>docker-compose exec postgres psql -U gateway -c \"SELECT version();\"\n</code></pre></p> <p>Permission errors: <pre><code>chmod 644 docker/config.yml\nchmod 600 docker/service_account.json\n</code></pre></p> <p>Rebuild after changes: <pre><code>docker-compose -f docker/docker-compose.yml up -d --build\n</code></pre></p>"},{"location":"gateway/docker-deployment/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide - Advanced configuration options</li> <li>Authentication - Set up API keys and user management</li> <li>Budget Management - Configure spending limits</li> <li>API Reference - Explore the complete API</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"gateway/overview/","title":"Gateway Overview","text":""},{"location":"gateway/overview/#what-is-any-llm-gateway","title":"What is any-llm-gateway?","text":"<p>any-llm-gateway is a FastAPI-based proxy server that adds production-grade budget enforcement, API key management, and usage analytics on top of any-llm's multi-provider foundation. It sits between your applications and LLM providers, giving you complete control over costs, access, and observability.</p>"},{"location":"gateway/overview/#why-use-the-gateway","title":"Why use the gateway?","text":"<p>Managing LLM costs and access at scale is challenging. Give users unrestricted access and you risk runaway costs. Lock it down too much and you slow down innovation. any-llm-gateway solves this by providing:</p> <ul> <li>Cost Control: Set budgets that automatically enforce or track spending limits</li> <li>Access Management: Issue, revoke, and monitor API keys generated for user access without exposing provider credentials</li> <li>Complete Visibility: Track every request with full token counts, costs, and metadata</li> <li>Production-Ready: Deploy with Docker and Postgres, Kubernetes-ready</li> </ul>"},{"location":"gateway/overview/#how-it-works","title":"How it works","text":"<p>The gateway acts as a transparent proxy between your applications and LLM providers. Here's the request flow:</p> <ol> <li>Your application sends a request to the gateway (instead of directly to OpenAI, Anthropic, etc.)</li> <li>The gateway authenticates the request, checks budget limits, and tracks usage</li> <li>The gateway routes to the appropriate provider based on the model format</li> <li>The provider processes the request and returns the response</li> <li> <p>The gateway logs the usage and returns the response to your application</p> <pre><code>curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"X-AnyLLM-Key: Bearer your-secure-master-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openai:gpt-5\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n</code></pre> <p>Learn how to set up your secure master key here </p> </li> </ol> <p> </p>"},{"location":"gateway/overview/#key-features","title":"Key Features","text":""},{"location":"gateway/overview/#smart-budget-management","title":"Smart Budget Management","text":"<p>Create shared budget tiers with automatic daily, weekly, or monthly resets. Budgets can be:</p> <ul> <li>Shared across multiple users - Perfect for team or organization-wide limits</li> <li>Automatically enforced - Requests are rejected when budgets are exceeded</li> <li>Tracking-only mode - Monitor spending without blocking requests</li> <li>Auto-resetting - No manual intervention required for recurring budgets</li> </ul> <p>Set up your first budget \u2192</p>"},{"location":"gateway/overview/#flexible-api-key-system","title":"Flexible API Key System","text":"<p>Choose between two authentication patterns:</p> <p>Master Key Authentication - Ideal for trusted services and internal tools - Full access to all gateway features</p> <p>Virtual API Keys - Create scoped keys with fine-grained control - Set expiration dates for time-limited access - Associate with users for spend tracking - Add custom metadata for tracking - Activate, deactivate, or revoke on demand</p> <p>Set up your keys \u2192</p>"},{"location":"gateway/overview/#complete-usage-analytics","title":"Complete Usage Analytics","text":"<p>Every request is logged with comprehensive details:</p> <ul> <li>Full token counts (prompt, completion, total)</li> <li>Per-request costs based on admin-configured per-token pricing</li> <li>Request metadata and timestamps</li> <li>User and API key attribution</li> </ul> <p>Track spending per user, view detailed usage history, and get the observability you need for cost attribution and chargebacks.</p>"},{"location":"gateway/overview/#production-ready-deployment","title":"Production-Ready Deployment","text":"<ul> <li>Quick Start: Deploy with Docker in minutes</li> <li>Flexible Configuration: Configure via YAML or environment variables</li> <li>Database: Designed for PostgreSQL</li> <li>Kubernetes Ready: Built-in liveness and readiness probes</li> </ul>"},{"location":"gateway/overview/#performance-impact","title":"Performance Impact","text":"<p>The gateway adds minimal latency (&lt;50ms) to requests while providing complete observability.</p>"},{"location":"gateway/overview/#getting-started","title":"Getting Started","text":"<p>For comprehensive setup instructions, see the Quick Start Guide.</p>"},{"location":"gateway/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Deploy and configure your first gateway</li> <li>Authentication - Set up master keys and virtual API keys</li> <li>Budget Management - Configure spending limits and tracking</li> <li>Configuration - Learn about all configuration options</li> <li>API Reference - Explore the complete API</li> </ul>"},{"location":"gateway/quickstart/","title":"Quick Start","text":"<p>This guide will help you set up any-llm-gateway and make your first LLM completion request. The gateway acts as a proxy between your applications and LLM providers, providing cost control, usage tracking, and API key management.</p> <p>By the end of this guide, you will:  </p> <ol> <li>Configure provider credentials and model pricing (e.g., OpenAI API key)  </li> <li>Run the gateway   </li> <li>Authenticate requests using a master key  </li> <li>Make completion requests through the gateway  </li> </ol> <p>Note: for the purposes of this quickstart we will utilize the docker-compose and config.yml file, but alternative configuration designs are available and detailed here</p>"},{"location":"gateway/quickstart/#pre-requisites","title":"Pre-Requisites","text":"<ol> <li>Docker</li> <li>Access to at least one LLM provider</li> </ol>"},{"location":"gateway/quickstart/#configure-and-run-the-gateway","title":"Configure and run the Gateway","text":"<p>When running any-llm-gateway, it must have a few things configured:</p> <ol> <li><code>GATEWAY_MASTER_KEY</code>. This master key has admin access to manage budgets, users, virtual keys, etc.</li> <li><code>DATABASE_URL</code>. The gateway relies upon a postgres database for storage.</li> <li>Provider Keys. The gateway connects to providers (Mistral, AWS, Vertex, Azure, etc) using credentials that must be set.</li> </ol>"},{"location":"gateway/quickstart/#create-a-project-directory","title":"Create a project directory","text":"<pre><code>mkdir any-llm-gateway\ncd any-llm-gateway\n</code></pre>"},{"location":"gateway/quickstart/#generate-master-key","title":"Generate  master key","text":"<p>First, generate a secure master key:  <pre><code>python -c \"import secrets; print(secrets.token_urlsafe(32))\"\n</code></pre></p> <p>Save the output of this command, you'll need it in the next steps. </p>"},{"location":"gateway/quickstart/#configure-providers","title":"Configure providers","text":"<p>Create a file name <code>config.yml</code> and paste the below content:</p> <p>Action :  At a minimum you'll need to fill out the master_key, and also enter credential information for at least one provider. You can browse supported providers here. If you would like to track usage cost, you'll also need to configure model pricing, as explained in the config template file.</p> <pre><code>database_url: \"postgresql://gateway:gateway@postgres:5432/gateway\"\n\nmaster_key: 09kS0xTiz6JqO....\n\nproviders:\n  openai:\n    api_key: YOUR_OPENAI_API_KEY_HERE\n    api_base: \"https://api.openai.com/v1\"  # optional, useful when you want to use a specific version of the API\n\nmodels:\n  openai:gpt-4:\n    input_price_per_million: 0.15\n    output_price_per_million: 0.60\n</code></pre>"},{"location":"gateway/quickstart/#set-up-docker-configuration","title":"Set up Docker Configuration","text":"<p>Create a file named <code>docker-compose.yml</code> with the following content.</p> <p>  Click to view docker-compose.yml content  <p><pre><code>services:\n  gateway:\n    # Use the official production image\n    image: ghcr.io/mozilla-ai/any-llm/gateway:latest\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./config.yml:/app/config.yml\n      # UNCOMMENT the next line ONLY if using Google Vertex AI (requires service_account.json)\n      # - ./service_account.json:/app/service_account.json\n    command: [\"any-llm-gateway\", \"serve\", \"--config\", \"/app/config.yml\"]\n    depends_on:\n      postgres:\n        condition: service_healthy\n    restart: unless-stopped\n\n  postgres:\n    image: postgres:16-alpine\n    environment:\n      - POSTGRES_USER=gateway\n      - POSTGRES_PASSWORD=gateway\n      - POSTGRES_DB=gateway\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U gateway\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n    restart: unless-stopped\n\nvolumes:\n  postgres_data:\n</code></pre> </p> <p>Alternatively, you can download the file directly from the repository: </p> <pre><code>curl -o docker-compose.yml https://raw.githubusercontent.com/mozilla-ai/any-llm/main/docker/docker-compose.yml\n</code></pre>"},{"location":"gateway/quickstart/#start-the-gateway","title":"Start the gateway","text":"<pre><code># From project root directory\ndocker compose up -d\n</code></pre> <pre><code># Verify the gateway is running\ncurl http://localhost:8000/health\n\n# Expected response:\n# {\"status\": \"healthy\"}\n</code></pre>"},{"location":"gateway/quickstart/#view-logs","title":"View Logs","text":"<pre><code>docker compose logs -f\n</code></pre>"},{"location":"gateway/quickstart/#create-a-user-and-make-your-first-request","title":"Create a user and make your first request","text":"<p>Now that it's running, clients can make requests! The gateway supports two authentication patterns: use of the master key, or virtual keys. See the authentication doc for more information. For this guide we will use the master key for both administration and client requests.</p> <p>To make the below commands easier to run, you can set the key as an env var in your terminal:</p> <pre><code>export GATEWAY_MASTER_KEY=YOUR_MASTER_KEY\n</code></pre> <p>tip: for the below <code>curl</code> commands, append <code>| jq</code> in order for it be pretty-printed in the console.</p>"},{"location":"gateway/quickstart/#create-a-user","title":"Create a user","text":"<p>To track usage, we must first create a user so that to associate our completion request.</p> <pre><code>curl -s -X POST http://localhost:8000/v1/users \\\n  -H \"X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"user_id\": \"user-123\", \"alias\": \"Bob\"}'\n</code></pre> Sample Response <pre><code>{\n    \"user_id\": \"user-123\",\n    \"alias\": \"Bob\",\n    \"spend\": 0,\n    \"budget_id\": null,\n    \"budget_started_at\": null,\n    \"next_budget_reset_at\": null,\n    \"blocked\": false,\n    \"created_at\": \"2025-11-07T16:41:44.429258+00:00\",\n    \"updated_at\": \"2025-11-07T16:41:44.429261+00:00\",\n    \"metadata\": {}\n}\n</code></pre>"},{"location":"gateway/quickstart/#make-a-request","title":"Make a request","text":"<p>Make a completion request using the master key and specify that the completion should be attached to the user you just created. This is only required when authenticating using the master key, if a user has a virtual key they do not need to specify a user id. You may also need to adjust the model to match one of the providers that you configured when running the gateway.</p> <pre><code>curl -s -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openai:gpt-4\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Write a haiku on Uranus!\"}],\n    \"user\": \"user-123\"\n  }'\n</code></pre> Sample Response  <pre><code>{\n    \"id\": \"chatcmpl-CZJvdiwHSdCZ2TfIPhutgPY4WYP46\",\n    \"choices\": [\n        {\n            \"finish_reason\": \"stop\",\n            \"index\": 0,\n            \"logprobs\": null,\n            \"message\": {\n                \"content\": \"Gleaming ice-blue sphere,\\nCircling in celestial dance,\\nUranus, so clear.\",\n                \"refusal\": null,\n                \"role\": \"assistant\",\n                \"annotations\": [],\n                \"audio\": null,\n                \"function_call\": null,\n                \"tool_calls\": null,\n                \"reasoning\": null\n            }\n        }\n    ],\n    \"created\": 1762534121,\n    \"model\": \"gpt-4-0613\",\n    \"object\": \"chat.completion\",\n    \"service_tier\": \"default\",\n    \"system_fingerprint\": null,\n    \"usage\": {\n        \"completion_tokens\": 21,\n        \"prompt_tokens\": 15,\n        \"total_tokens\": 36,\n        \"completion_tokens_details\": {\n            \"accepted_prediction_tokens\": 0,\n            \"audio_tokens\": 0,\n            \"reasoning_tokens\": 0,\n            \"rejected_prediction_tokens\": 0\n        },\n        \"prompt_tokens_details\": {\n            \"audio_tokens\": 0,\n            \"cached_tokens\": 0\n        }\n    }\n}\n</code></pre> <p>Alternatively, if you are using the any-llm python sdk, you can access using the gateway client.</p> <pre><code>import os\nfrom any_llm import completion\n\ncompletion(\n  provider=\"gateway\",\n  model=\"openai:gpt-4\",\n  api_base=\"http://localhost:8000/v1\",\n  api_key=os.environ['GATEWAY_MASTER_KEY'],\n  messages=[{\"role\": \"user\", \"content\": \"Write a haiku on Uranus!\"}],\n  user=\"user-123\",\n)\n</code></pre>"},{"location":"gateway/quickstart/#view-metrics","title":"View metrics","text":"<p>Now using the master key, we can access the usage information for the user.</p> <pre><code>curl -s http://localhost:8000/v1/users/user-123 \\\n  -H \"X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}\" \\\n  -H \"Content-Type: application/json\"\n</code></pre> Sample Response  <pre><code>{\n    \"user_id\": \"user-123\",\n    \"alias\": \"Bob\",\n    \"spend\": 0.0000216,\n    \"budget_id\": null,\n    \"budget_started_at\": null,\n    \"next_budget_reset_at\": null,\n    \"blocked\": false,\n    \"created_at\": \"2025-11-07T16:41:44.429258+00:00\",\n    \"updated_at\": \"2025-11-07T16:48:42.972327+00:00\",\n    \"metadata\": {}\n}\n</code></pre> <p>You'll notice that the user does not have a budget attached, which means that we track their usage but do not limit them! For more information on creating and managing budgets and budget reset cycles, see the Budget Management docs</p>"},{"location":"gateway/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - Configure providers, pricing, and other settings</li> <li>Authentication - Learn about master keys and virtual API keys</li> <li>Budget Management - Set spending limits and track costs</li> <li>API Reference - Explore the complete API</li> </ul>"},{"location":"gateway/troubleshooting/","title":"Troubleshooting","text":""},{"location":"gateway/troubleshooting/#database-connection-errors","title":"Database connection errors","text":"<p>Make sure the database URL is correct and the database is accessible:</p> <pre><code>python -c \"from sqlalchemy import create_engine; engine = create_engine('postgresql://user:pass@host/db'); print('OK')\"\n</code></pre>"},{"location":"gateway/troubleshooting/#common-issues","title":"Common Issues","text":""},{"location":"gateway/troubleshooting/#authentication-errors","title":"Authentication Errors","text":"<ul> <li>Ensure you're using the correct master key format: <code>Bearer your-secure-master-key</code></li> <li>Check that the <code>X-AnyLLM-Key</code> header is properly set</li> <li>Verify that virtual API keys are active and not expired</li> </ul>"},{"location":"gateway/troubleshooting/#configuration-issues","title":"Configuration Issues","text":"<ul> <li>Verify your <code>config.yml</code> file is properly formatted</li> <li>Check that environment variables are set correctly</li> <li>Ensure provider API keys are valid and have proper permissions</li> </ul>"},{"location":"gateway/troubleshooting/#budget-enforcement","title":"Budget Enforcement","text":"<ul> <li>Check that budgets are properly assigned to users</li> <li>Verify budget limits are set correctly</li> <li>Monitor user spending to ensure limits are being enforced</li> </ul>"},{"location":"gateway/troubleshooting/#getting-help","title":"Getting Help","text":"<ul> <li>Check the logs for detailed error messages</li> <li>Verify your configuration matches the examples in the documentation</li> <li>Ensure all required environment variables are set</li> </ul>"},{"location":"platform/overview/","title":"Managed Platform Overview","text":""},{"location":"platform/overview/#what-is-the-any-llm-managed-platform","title":"What is the any-llm Managed Platform?","text":"<p>The any-llm managed platform is a cloud-hosted service that provides secure API key vaulting and usage tracking for all your LLM providers. Instead of managing multiple provider API keys across your codebase, you get a single virtual key that works with any supported provider while keeping your credentials encrypted and your usage tracked.</p> <p>The managed platform is available at any-llm.ai.</p>"},{"location":"platform/overview/#why-use-the-managed-platform","title":"Why use the Managed Platform?","text":"<p>Managing LLM API keys and tracking costs across multiple providers is challenging:</p> <ul> <li>Security risks: API keys scattered across <code>.env</code> files, CI/CD pipelines, and developer machines</li> <li>No visibility: Difficult to track spending across OpenAI, Anthropic, Google, and other providers</li> <li>Key rotation pain: Updating keys means touching multiple systems and codebases</li> <li>No performance insights: No easy way to measure latency, throughput, or reliability</li> </ul> <p>The managed platform solves these problems:</p> <ul> <li>Secure Key Vault: Your provider API keys are encrypted client-side before storage\u2014we never see your raw keys</li> <li>Single Virtual Key: One <code>ANY_LLM_KEY</code> works across all providers</li> <li>Usage Analytics: Track tokens, costs, and performance metrics without logging prompts or responses</li> <li>Zero Infrastructure: No servers to deploy, no databases to manage</li> </ul>"},{"location":"platform/overview/#how-it-works","title":"How it works","text":"<p>The managed platform acts as a secure credential manager and usage tracker. Here's the flow:</p> <ol> <li>You add provider keys to the platform dashboard (keys are encrypted in your browser before upload)</li> <li>You get a virtual key (<code>ANY_LLM_KEY</code>) that represents your project</li> <li>Your application uses the <code>PlatformProvider</code> with your virtual key</li> <li>The SDK authenticates with the platform, retrieves and decrypts your provider key client-side</li> <li>Your request goes directly to the LLM provider (OpenAI, Anthropic, etc.)</li> <li>Usage metadata (tokens, model, latency) is reported back\u2014never your prompts or responses</li> </ol> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          Your Application                               \u2502\n\u2502                                                                         \u2502\n\u2502   from any_llm import completion                                        \u2502\n\u2502   completion(provider=\"platform\", model=\"openai:gpt-4\", ...)            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        any-llm SDK (PlatformProvider)                   \u2502\n\u2502                                                                         \u2502\n\u2502  1. Authenticate with platform using ANY_LLM_KEY                        \u2502\n\u2502  2. Receive encrypted provider key                                      \u2502\n\u2502  3. Decrypt provider key locally (client-side)                          \u2502\n\u2502  4. Make request directly to provider                                   \u2502\n\u2502  5. Report usage metadata (tokens, latency) to platform                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502                                     \u2502\n                 \u25bc                                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   any-llm Managed Platform  \u2502       \u2502        LLM Provider                \u2502\n\u2502                             \u2502       \u2502   (OpenAI, Anthropic, etc.)        \u2502\n\u2502  \u2022 Encrypted key storage    \u2502       \u2502                                    \u2502\n\u2502  \u2022 Usage tracking           \u2502       \u2502   Your prompts/responses go        \u2502\n\u2502  \u2022 Cost analytics           \u2502       \u2502   directly here\u2014never through      \u2502\n\u2502  \u2022 Performance metrics      \u2502       \u2502   our platform                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"platform/overview/#key-features","title":"Key Features","text":""},{"location":"platform/overview/#client-side-encryption","title":"Client-Side Encryption","text":"<p>Your provider API keys are encrypted in your browser using XChaCha20-Poly1305 before being sent to our servers. The encryption key is derived from your account credentials and never leaves your device. This means:</p> <ul> <li>We cannot read your provider API keys</li> <li>Even if our database were compromised, your keys remain encrypted</li> <li>You maintain full control over your credentials</li> </ul>"},{"location":"platform/overview/#privacy-first-usage-tracking","title":"Privacy-First Usage Tracking","text":"<p>The platform tracks usage metadata to provide cost and performance insights:</p> <p>What we track for you:</p> <ul> <li>Token counts (input and output)</li> <li>Model name and provider</li> <li>Request timestamps</li> <li>Performance metrics (latency, throughput)</li> </ul> <p>What we never track:</p> <ul> <li>Your prompts</li> <li>Model responses</li> <li>Any content from your conversations</li> </ul>"},{"location":"platform/overview/#project-organization","title":"Project Organization","text":"<p>Organize your usage by project, team, or environment:</p> <ul> <li>Create separate projects for development, staging, and production</li> <li>Track costs per project</li> <li>Set up different provider keys per project</li> </ul>"},{"location":"platform/overview/#platform-vs-gateway","title":"Platform vs. Gateway","text":"<p>any-llm offers two solutions for managing LLM access. Choose the one that fits your needs:</p> Feature Managed Platform Self-Hosted Gateway Deployment Cloud-hosted (no infrastructure) Self-hosted (Docker + Postgres) Key Storage Client-side encrypted vault Your own configuration Budget Enforcement Coming soon Built-in User Management Per-project Full user/key management Request Routing Direct to provider, no proxy Through your gateway Best For Teams wanting zero-ops key management and usage tracking Organizations needing full control <p>You can also use both together\u2014store your provider keys in the managed platform and use them in a self-hosted gateway deployment.</p>"},{"location":"platform/overview/#current-status","title":"Current Status","text":"<p>The any-llm managed platform is in open beta. During the beta:</p> <ul> <li>Free access to all features</li> <li>Core encryption and key management are production-ready</li> <li>Dashboard UX and advanced features are being refined</li> <li>Feedback is welcome at any-llm.ai</li> </ul>"},{"location":"platform/overview/#getting-started","title":"Getting Started","text":"<p>Ready to try the managed platform?</p> <ol> <li>Create an account at any-llm.ai</li> <li>Add your provider API keys</li> <li>Get your virtual key</li> <li>Make your first request</li> </ol>"}]}